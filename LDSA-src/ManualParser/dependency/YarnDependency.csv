ID,Option A,Option B,Option A Description,Option B Description
1,yarn.resourcemanager.bind-host,yarn.resourcemanager.address,"The actual address the server will bind to. If this optional address is set, the RPC and webapp servers will bind to this address and the port specified in yarn.resourcemanager.address and yarn.resourcemanager.webapp.address, respectively. This is most useful for making RM listen to all interfaces by setting to 0.0.0.0.",The address of the applications manager interface in the RM.
2,yarn.resourcemanager.bind-host,yarn.resourcemanager.webapp.address,"The actual address the server will bind to. If this optional address is set, the RPC and webapp servers will bind to this address and the port specified in yarn.resourcemanager.address and yarn.resourcemanager.webapp.address, respectively. This is most useful for making RM listen to all interfaces by setting to 0.0.0.0.","The http address of the RM web application. If only a host is provided as the value, the webapp will be served on a random port."
3,yarn.resourcemanager.am.max-attempts,yarn.resourcemanager.am.global.max-attempts,"The default maximum number of application attempts, if unset by the user. Each application master can specify its individual maximum number of application attempts via the API, but the individual number cannot be more than the global upper bound in yarn.resourcemanager.am.global.max-attempts. The default number is set to 2, to allow at least one retry for AM.","The maximum number of application attempts. It's a global setting for all application masters. Each application master can specify its individual maximum number of application attempts via the API, but the individual number cannot be more than the global upper bound. If it is, the resourcemanager will override it. The default number value is set to yarn.resourcemanager.am.max-attempts."
4,yarn.resourcemanager.recovery.enabled,yarn.resourcemanager.store.class,"Enable RM to recover state after starting. If true, then yarn.resourcemanager.store.class must be specified.","The class to use as the persistent store. If org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore is used, the store is implicitly fenced; meaning a single ResourceManager is able to use the store at any point in time. More details on this implicit fencing, along with setting up appropriate ACLs is discussed under yarn.resourcemanager.zk-state-store.root-node.acl."
5,yarn.resourcemanager.fail-fast,yarn.fail-fast,"Should RM fail fast if it encounters any errors. By defalt, it points to ${yarn.fail-fast}. Errors include: 1) exceptions when state-store write/read operations fails.","Should YARN fail fast if it encounters any errors. This is a global config for all other components including RM,NM etc. If no value is set for component-specific config (e.g yarn.resourcemanager.fail-fast), this value will be the default."
6,yarn.fail-fast,yarn.resourcemanager.fail-fast,"Should YARN fail fast if it encounters any errors. This is a global config for all other components including RM,NM etc. If no value is set for component-specific config (e.g yarn.resourcemanager.fail-fast), this value will be the default.","Should RM fail fast if it encounters any errors. By defalt, it points to ${yarn.fail-fast}. Errors include: 1) exceptions when state-store write/read operations fails."
7,yarn.resourcemanager.store.class,yarn.resourcemanager.zk-state-store.root-node.acl,"The class to use as the persistent store. If org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore is used, the store is implicitly fenced; meaning a single ResourceManager is able to use the store at any point in time. More details on this implicit fencing, along with setting up appropriate ACLs is discussed under yarn.resourcemanager.zk-state-store.root-node.acl.","ACLs to be used for the root znode when using ZKRMStateStore in an HA scenario for fencing. ZKRMStateStore supports implicit fencing to allow a single ResourceManager write-access to the store. For fencing, the ResourceManagers in the cluster share read-write-admin privileges on the root node, but the Active ResourceManager claims exclusive create-delete permissions. By default, when this property is not set, we use the ACLs from yarn.resourcemanager.zk-acl for shared admin access and rm-address:random-number for username-based exclusive create-delete access. This property allows users to set ACLs of their choice instead of using the default mechanism. For fencing to work, the ACLs should be carefully set differently on each ResourceManager such that all the ResourceManagers have shared admin access and the Active ResourceManager takes over (exclusively) the create-delete access."
8,yarn.resourcemanager.state-store.max-completed-applications,yarn.resourcemanager.max-completed-applications,"The maximum number of completed applications RM state store keeps, less than or equals to ${yarn.resourcemanager.max-completed-applications}. By default, it equals to ${yarn.resourcemanager.max-completed-applications}. This ensures that the applications kept in the state store are consistent with the applications remembered in RM memory. Any values larger than ${yarn.resourcemanager.max-completed-applications} will be reset to ${yarn.resourcemanager.max-completed-applications}. Note that this value impacts the RM recovery performance. Typically, a smaller value indicates better performance on RM recovery.",The maximum number of completed applications RM keeps.
9,yarn.resourcemanager.zk-state-store.parent-path,yarn.resourcemanager.store.class,Full path of the ZooKeeper znode where RM state will be stored. This must be supplied when using org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore as the value for yarn.resourcemanager.store.class,"The class to use as the persistent store. If org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore is used, the store is implicitly fenced; meaning a single ResourceManager is able to use the store at any point in time. More details on this implicit fencing, along with setting up appropriate ACLs is discussed under yarn.resourcemanager.zk-state-store.root-node.acl."
10,yarn.resourcemanager.fs.state-store.uri,yarn.resourcemanager.store.class,URI pointing to the location of the FileSystem path where RM state will be stored. This must be supplied when using org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore as the value for yarn.resourcemanager.store.class,"The class to use as the persistent store. If org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore is used, the store is implicitly fenced; meaning a single ResourceManager is able to use the store at any point in time. More details on this implicit fencing, along with setting up appropriate ACLs is discussed under yarn.resourcemanager.zk-state-store.root-node.acl."
11,yarn.resourcemanager.leveldb-state-store.path,yarn.resourcemanager.store.class,Local path where the RM state will be stored when using org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore as the value for yarn.resourcemanager.store.class,"The class to use as the persistent store. If org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore is used, the store is implicitly fenced; meaning a single ResourceManager is able to use the store at any point in time. More details on this implicit fencing, along with setting up appropriate ACLs is discussed under yarn.resourcemanager.zk-state-store.root-node.acl."
12,yarn.resourcemanager.ha.enabled,yarn.resourcemanager.address,"Enable RM high-availability. When enabled, (1) The RM starts in the Standby mode by default, and transitions to the Active mode when prompted to. (2) The nodes in the RM ensemble are listed in yarn.resourcemanager.ha.rm-ids (3) The id of each RM either comes from yarn.resourcemanager.ha.id if yarn.resourcemanager.ha.id is explicitly specified or can be figured out by matching yarn.resourcemanager.address.{id} with local address (4) The actual physical addresses come from the configs of the pattern - {rpc-config}.{id}",The address of the applications manager interface in the RM.
13,yarn.resourcemanager.ha.enabled,yarn.resourcemanager.ha.rm-ids,"Enable RM high-availability. When enabled, (1) The RM starts in the Standby mode by default, and transitions to the Active mode when prompted to. (2) The nodes in the RM ensemble are listed in yarn.resourcemanager.ha.rm-ids (3) The id of each RM either comes from yarn.resourcemanager.ha.id if yarn.resourcemanager.ha.id is explicitly specified or can be figured out by matching yarn.resourcemanager.address.{id} with local address (4) The actual physical addresses come from the configs of the pattern - {rpc-config}.{id}",The list of RM nodes in the cluster when HA is enabled. See description of yarn.resourcemanager.ha .enabled for full details on how this is used.
14,yarn.resourcemanager.ha.enabled,yarn.resourcemanager.ha.id,"Enable RM high-availability. When enabled, (1) The RM starts in the Standby mode by default, and transitions to the Active mode when prompted to. (2) The nodes in the RM ensemble are listed in yarn.resourcemanager.ha.rm-ids (3) The id of each RM either comes from yarn.resourcemanager.ha.id if yarn.resourcemanager.ha.id is explicitly specified or can be figured out by matching yarn.resourcemanager.address.{id} with local address (4) The actual physical addresses come from the configs of the pattern - {rpc-config}.{id}","The id (string) of the current RM. When HA is enabled, this is an optional config. The id of current RM can be set by explicitly specifying yarn.resourcemanager.ha.id or figured out by matching yarn.resourcemanager.address.{id} with local address See description of yarn.resourcemanager.ha.enabled for full details on how this is used."
15,yarn.resourcemanager.ha.id,yarn.resourcemanager.address,"The id (string) of the current RM. When HA is enabled, this is an optional config. The id of current RM can be set by explicitly specifying yarn.resourcemanager.ha.id or figured out by matching yarn.resourcemanager.address.{id} with local address See description of yarn.resourcemanager.ha.enabled for full details on how this is used.",The address of the applications manager interface in the RM.
16,yarn.resourcemanager.ha.id,yarn.resourcemanager.ha.enabled,"The id (string) of the current RM. When HA is enabled, this is an optional config. The id of current RM can be set by explicitly specifying yarn.resourcemanager.ha.id or figured out by matching yarn.resourcemanager.address.{id} with local address See description of yarn.resourcemanager.ha.enabled for full details on how this is used.","Enable RM high-availability. When enabled, (1) The RM starts in the Standby mode by default, and transitions to the Active mode when prompted to. (2) The nodes in the RM ensemble are listed in yarn.resourcemanager.ha.rm-ids (3) The id of each RM either comes from yarn.resourcemanager.ha.id if yarn.resourcemanager.ha.id is explicitly specified or can be figured out by matching yarn.resourcemanager.address.{id} with local address (4) The actual physical addresses come from the configs of the pattern - {rpc-config}.{id}"
17,yarn.client.failover-max-attempts,yarn.resourcemanager.connect.max-wait.ms,"When HA is enabled, the max number of times FailoverProxyProvider should attempt failover. When set, this overrides the yarn.resourcemanager.connect.max-wait.ms. When not set, this is inferred from yarn.resourcemanager.connect.max-wait.ms.",Maximum time to wait to establish connection to ResourceManager.
18,yarn.client.failover-sleep-base-ms,yarn.resourcemanager.connect.retry-interval.ms,"When HA is enabled, the sleep base (in milliseconds) to be used for calculating the exponential delay between failovers. When set, this overrides the yarn.resourcemanager.connect.* settings. When not set, yarn.resourcemanager.connect.retry-interval.ms is used instead.",How often to try connecting to the ResourceManager.
19,yarn.client.failover-sleep-max-ms,yarn.resourcemanager.connect.retry-interval.ms,"When HA is enabled, the maximum sleep time (in milliseconds) between failovers. When set, this overrides the yarn.resourcemanager.connect.* settings. When not set, yarn.resourcemanager.connect.retry-interval.ms is used instead.",How often to try connecting to the ResourceManager.
20,yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs,yarn.nm.liveness-monitor.expiry-interval-ms,Interval for the roll over for the master key used to generate container tokens. It is expected to be much greater than yarn.nm.liveness-monitor.expiry-interval-ms and yarn.resourcemanager.rm.container-allocation.expiry-interval-ms. Otherwise the behavior is undefined.,How long to wait until a node manager is considered dead.
21,yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs,yarn.resourcemanager.rm.container-allocation.expiry-interval-ms,Interval for the roll over for the master key used to generate container tokens. It is expected to be much greater than yarn.nm.liveness-monitor.expiry-interval-ms and yarn.resourcemanager.rm.container-allocation.expiry-interval-ms. Otherwise the behavior is undefined.,The expiry interval for a container
22,yarn.resourcemanager.scheduler.monitor.enable,yarn.resourcemanager.scheduler.monitor.policies,Enable a set of periodic monitors (specified in yarn.resourcemanager.scheduler.monitor.policies) that affect the scheduler.,"The list of SchedulingEditPolicy classes that interact with the scheduler. A particular module may be incompatible with the scheduler, other policies, or a configuration of either."
23,yarn.resourcemanager.configuration.file-system-based-store,yarn.resourcemanager.configuration.provider-class,The value specifies the file system (e.g. HDFS) path where ResourceManager loads configuration if yarn.resourcemanager.configuration.provider-class is set to org.apache.hadoop.yarn.FileSystemBasedConfigurationProvider.,"The class to use as the configuration provider. If org.apache.hadoop.yarn.LocalConfigurationProvider is used, the local configuration will be loaded. If org.apache.hadoop.yarn.FileSystemBasedConfigurationProvider is used, the configuration which will be loaded should be uploaded to remote File system first."
24,yarn.resourcemanager.system-metrics-publisher.enabled,yarn.system-metrics-publisher.enabled,"The setting that controls whether yarn system metrics is published to the Timeline server (version one) or not, by RM. This configuration is now deprecated in favor of yarn.system-metrics-publisher.enabled.",The setting that controls whether yarn system metrics is published on the Timeline service or not by RM And NM.
25,yarn.resourcemanager.nm-tokens.master-key-rolling-interval-secs,yarn.nm.liveness-monitor.expiry-interval-ms,Interval for the roll over for the master key used to generate NodeManager tokens. It is expected to be set to a value much larger than yarn.nm.liveness-monitor.expiry-interval-ms.,How long to wait until a node manager is considered dead.
26,yarn.nodemanager.bind-host,yarn.nodemanager.address,"The actual address the server will bind to. If this optional address is set, the RPC and webapp servers will bind to this address and the port specified in yarn.nodemanager.address and yarn.nodemanager.webapp.address, respectively. This is most useful for making NM listen to all interfaces by setting to 0.0.0.0.",The address of the container manager in the NM.
27,yarn.nodemanager.bind-host,yarn.nodemanager.webapp.address,"The actual address the server will bind to. If this optional address is set, the RPC and webapp servers will bind to this address and the port specified in yarn.nodemanager.address and yarn.nodemanager.webapp.address, respectively. This is most useful for making NM listen to all interfaces by setting to 0.0.0.0.",NM Webapp address.
28,yarn.nodemanager.delete.debug-delay-sec,yarn.nodemanager.local-dirs,"Number of seconds after an application finishes before the nodemanager's DeletionService will delete the application's localized file directory and log directory. To diagnose YARN application problems, set this property's value large enough (for example, to 600 = 10 minutes) to permit examination of these directories. After changing the property's value, you must restart the nodemanager in order for it to have an effect. The roots of YARN applications' work directories is configurable with the yarn.nodemanager.local-dirs property (see below), and the roots of the YARN applications' log directories is configurable with the yarn.nodemanager.log-dirs property (see also below).","List of directories to store localized files in. An application's localized file directory will be found in: ${yarn.nodemanager.local-dirs}/usercache/${user}/appcache/application_${appid}. Individual containers' work directories, called container_${contid}, will be subdirectories of this."
29,yarn.nodemanager.delete.debug-delay-sec,yarn.nodemanager.log-dirs,"Number of seconds after an application finishes before the nodemanager's DeletionService will delete the application's localized file directory and log directory. To diagnose YARN application problems, set this property's value large enough (for example, to 600 = 10 minutes) to permit examination of these directories. After changing the property's value, you must restart the nodemanager in order for it to have an effect. The roots of YARN applications' work directories is configurable with the yarn.nodemanager.local-dirs property (see below), and the roots of the YARN applications' log directories is configurable with the yarn.nodemanager.log-dirs property (see also below).","Where to store container logs. An application's localized log directory will be found in ${yarn.nodemanager.log-dirs}/application_${appid}. Individual containers' log directories will be below this, in directories named container_{$contid}. Each container directory will contain the files stderr, stdin, and syslog generated by that container."
30,yarn.log-aggregation-enable,yarn.nodemanager.remote-app-log-dir,"Whether to enable log aggregation. Log aggregation collects each container's logs and moves these logs onto a file-system, for e.g. HDFS, after the application completes. Users can configure the ""yarn.nodemanager.remote-app-log-dir"" and ""yarn.nodemanager.remote-app-log-dir-suffix"" properties to determine where these logs are moved to. Users can access the logs via the Application Timeline Server.",Where to aggregate logs to.
31,yarn.log-aggregation-enable,yarn.nodemanager.remote-app-log-dir-suffix,"Whether to enable log aggregation. Log aggregation collects each container's logs and moves these logs onto a file-system, for e.g. HDFS, after the application completes. Users can configure the ""yarn.nodemanager.remote-app-log-dir"" and ""yarn.nodemanager.remote-app-log-dir-suffix"" properties to determine where these logs are moved to. Users can access the logs via the Application Timeline Server.",The remote log dir will be created at {yarn.nodemanager.remote-app-log-dir}/${user}/{thisParam}
32,yarn.nodemanager.remote-app-log-dir-suffix,yarn.nodemanager.remote-app-log-dir,The remote log dir will be created at {yarn.nodemanager.remote-app-log-dir}/${user}/{thisParam},Where to aggregate logs to.
33,yarn.nodemanager.resource.memory-mb,yarn.nodemanager.resource.detect-hardware-capabilities,"Amount of physical memory, in MB, that can be allocated for containers. If set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true, it is automatically calculated(in case of Windows and Linux). In other cases, the default is 8192MB.",Enable auto-detection of node capabilities such as memory and CPU.
34,yarn.nodemanager.resource.system-reserved-memory-mb,yarn.nodemanager.resource.memory-mb,"Amount of physical memory, in MB, that is reserved for non-YARN processes. This configuration is only used if yarn.nodemanager.resource.detect-hardware-capabilities is set to true and yarn.nodemanager.resource.memory-mb is -1. If set to -1, this amount is calculated as 20% of (system memory - 2*HADOOP_HEAPSIZE)","Amount of physical memory, in MB, that can be allocated for containers. If set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true, it is automatically calculated(in case of Windows and Linux). In other cases, the default is 8192MB."
35,yarn.nodemanager.resource.system-reserved-memory-mb,yarn.nodemanager.resource.detect-hardware-capabilities,"Amount of physical memory, in MB, that is reserved for non-YARN processes. This configuration is only used if yarn.nodemanager.resource.detect-hardware-capabilities is set to true and yarn.nodemanager.resource.memory-mb is -1. If set to -1, this amount is calculated as 20% of (system memory - 2*HADOOP_HEAPSIZE)",Enable auto-detection of node capabilities such as memory and CPU.
36,yarn.nodemanager.resource.cpu-vcores,yarn.nodemanager.resource.detect-hardware-capabilities,"Number of vcores that can be allocated for containers. This is used by the RM scheduler when allocating resources for containers. This is not used to limit the number of CPUs used by YARN containers. If it is set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true, it is automatically determined from the hardware in case of Windows and Linux. In other cases, number of vcores is 8 by default.",Enable auto-detection of node capabilities such as memory and CPU.
37,yarn.nodemanager.resource.count-logical-processors-as-cores,yarn.nodemanager.resource.cpu-vcores,Flag to determine if logical processors(such as hyperthreads) should be counted as cores. Only applicable on Linux when yarn.nodemanager.resource.cpu-vcores is set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true.,"Number of vcores that can be allocated for containers. This is used by the RM scheduler when allocating resources for containers. This is not used to limit the number of CPUs used by YARN containers. If it is set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true, it is automatically determined from the hardware in case of Windows and Linux. In other cases, number of vcores is 8 by default."
38,yarn.nodemanager.resource.count-logical-processors-as-cores,yarn.nodemanager.resource.detect-hardware-capabilities,Flag to determine if logical processors(such as hyperthreads) should be counted as cores. Only applicable on Linux when yarn.nodemanager.resource.cpu-vcores is set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true.,Enable auto-detection of node capabilities such as memory and CPU.
39,yarn.nodemanager.resource.pcores-vcores-multiplier,yarn.nodemanager.resource.cpu-vcores,Multiplier to determine how to convert phyiscal cores to vcores. This value is used if yarn.nodemanager.resource.cpu-vcores is set to -1(which implies auto-calculate vcores) and yarn.nodemanager.resource.detect-hardware-capabilities is set to true. The number of vcores will be calculated as number of CPUs * multiplier.,"Number of vcores that can be allocated for containers. This is used by the RM scheduler when allocating resources for containers. This is not used to limit the number of CPUs used by YARN containers. If it is set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true, it is automatically determined from the hardware in case of Windows and Linux. In other cases, number of vcores is 8 by default."
40,yarn.nodemanager.resource.pcores-vcores-multiplier,yarn.nodemanager.resource.detect-hardware-capabilities,Multiplier to determine how to convert phyiscal cores to vcores. This value is used if yarn.nodemanager.resource.cpu-vcores is set to -1(which implies auto-calculate vcores) and yarn.nodemanager.resource.detect-hardware-capabilities is set to true. The number of vcores will be calculated as number of CPUs * multiplier.,Enable auto-detection of node capabilities such as memory and CPU.
41,yarn.nodemanager.container-monitor.interval-ms,yarn.nodemanager.resource-monitor.interval-ms,"How often to monitor containers. If not set, the value for yarn.nodemanager.resource-monitor.interval-ms will be used. If 0 or negative, container monitoring is disabled.","How often to monitor the node and the containers. If 0 or negative, monitoring is disabled."
42,yarn.nodemanager.container-monitor.resource-calculator.class,yarn.nodemanager.resource-calculator.class,"Class that calculates containers current resource utilization. If not set, the value for yarn.nodemanager.resource-calculator.class will be used.",Class that calculates current resource utilization.
43,yarn.nodemanager.disk-health-checker.min-healthy-disks,yarn.nodemanager.local-dirs,"The minimum fraction of number of disks to be healthy for the nodemanager to launch new containers. This correspond to both yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs. i.e. If there are less number of healthy local-dirs (or log-dirs) available, then new containers will not be launched on this node.","List of directories to store localized files in. An application's localized file directory will be found in: ${yarn.nodemanager.local-dirs}/usercache/${user}/appcache/application_${appid}. Individual containers' work directories, called container_${contid}, will be subdirectories of this."
44,yarn.nodemanager.disk-health-checker.min-healthy-disks,yarn.nodemanager.log-dirs,"The minimum fraction of number of disks to be healthy for the nodemanager to launch new containers. This correspond to both yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs. i.e. If there are less number of healthy local-dirs (or log-dirs) available, then new containers will not be launched on this node.","Where to store container logs. An application's localized log directory will be found in ${yarn.nodemanager.log-dirs}/application_${appid}. Individual containers' log directories will be below this, in directories named container_{$contid}. Each container directory will contain the files stderr, stdin, and syslog generated by that container."
45,yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage,yarn.nodemanager.local-dirs,"The maximum percentage of disk space utilization allowed after which a disk is marked as bad. Values can range from 0.0 to 100.0. If the value is greater than or equal to 100, the nodemanager will check for full disk. This applies to yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs when yarn.nodemanager.disk-health-checker.disk-utilization-threshold.enabled is true.","List of directories to store localized files in. An application's localized file directory will be found in: ${yarn.nodemanager.local-dirs}/usercache/${user}/appcache/application_${appid}. Individual containers' work directories, called container_${contid}, will be subdirectories of this."
46,yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage,yarn.nodemanager.log-dirs,"The maximum percentage of disk space utilization allowed after which a disk is marked as bad. Values can range from 0.0 to 100.0. If the value is greater than or equal to 100, the nodemanager will check for full disk. This applies to yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs when yarn.nodemanager.disk-health-checker.disk-utilization-threshold.enabled is true.","Where to store container logs. An application's localized log directory will be found in ${yarn.nodemanager.log-dirs}/application_${appid}. Individual containers' log directories will be below this, in directories named container_{$contid}. Each container directory will contain the files stderr, stdin, and syslog generated by that container."
47,yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage,yarn.nodemanager.disk-health-checker.disk-utilization-threshold.enabled,"The maximum percentage of disk space utilization allowed after which a disk is marked as bad. Values can range from 0.0 to 100.0. If the value is greater than or equal to 100, the nodemanager will check for full disk. This applies to yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs when yarn.nodemanager.disk-health-checker.disk-utilization-threshold.enabled is true.",Enable/Disable the disk utilisation percentage threshold for disk health checker.
48,yarn.nodemanager.disk-health-checker.disk-utilization-watermark-low-per-disk-percentage,yarn.nodemanager.local-dirs,"The low threshold percentage of disk space used when a bad disk is marked as good. Values can range from 0.0 to 100.0. This applies to yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs. Note that if its value is more than yarn.nodemanager.disk-health-checker. max-disk-utilization-per-disk-percentage or not set, it will be set to the same value as yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage.","List of directories to store localized files in. An application's localized file directory will be found in: ${yarn.nodemanager.local-dirs}/usercache/${user}/appcache/application_${appid}. Individual containers' work directories, called container_${contid}, will be subdirectories of this."
49,yarn.nodemanager.disk-health-checker.disk-utilization-watermark-low-per-disk-percentage,yarn.nodemanager.log-dirs,"The low threshold percentage of disk space used when a bad disk is marked as good. Values can range from 0.0 to 100.0. This applies to yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs. Note that if its value is more than yarn.nodemanager.disk-health-checker. max-disk-utilization-per-disk-percentage or not set, it will be set to the same value as yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage.","Where to store container logs. An application's localized log directory will be found in ${yarn.nodemanager.log-dirs}/application_${appid}. Individual containers' log directories will be below this, in directories named container_{$contid}. Each container directory will contain the files stderr, stdin, and syslog generated by that container."
50,yarn.nodemanager.disk-health-checker.disk-utilization-watermark-low-per-disk-percentage,yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage,"The low threshold percentage of disk space used when a bad disk is marked as good. Values can range from 0.0 to 100.0. This applies to yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs. Note that if its value is more than yarn.nodemanager.disk-health-checker. max-disk-utilization-per-disk-percentage or not set, it will be set to the same value as yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage.","The maximum percentage of disk space utilization allowed after which a disk is marked as bad. Values can range from 0.0 to 100.0. If the value is greater than or equal to 100, the nodemanager will check for full disk. This applies to yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs when yarn.nodemanager.disk-health-checker.disk-utilization-threshold.enabled is true."
51,yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb,yarn.nodemanager.local-dirs,"The minimum space in megabytes that must be available on a disk for it to be used. If space on a disk falls below this threshold, it will be marked as bad. This applies to yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs when yarn.nodemanager.disk-health-checker.disk-free-space-threshold.enabled is true.","List of directories to store localized files in. An application's localized file directory will be found in: ${yarn.nodemanager.local-dirs}/usercache/${user}/appcache/application_${appid}. Individual containers' work directories, called container_${contid}, will be subdirectories of this."
52,yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb,yarn.nodemanager.log-dirs,"The minimum space in megabytes that must be available on a disk for it to be used. If space on a disk falls below this threshold, it will be marked as bad. This applies to yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs when yarn.nodemanager.disk-health-checker.disk-free-space-threshold.enabled is true.","Where to store container logs. An application's localized log directory will be found in ${yarn.nodemanager.log-dirs}/application_${appid}. Individual containers' log directories will be below this, in directories named container_{$contid}. Each container directory will contain the files stderr, stdin, and syslog generated by that container."
53,yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb,yarn.nodemanager.disk-health-checker.disk-free-space-threshold.enabled,"The minimum space in megabytes that must be available on a disk for it to be used. If space on a disk falls below this threshold, it will be marked as bad. This applies to yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs when yarn.nodemanager.disk-health-checker.disk-free-space-threshold.enabled is true.",Enable/Disable the minimum disk free space threshold for disk health checker.
54,yarn.nodemanager.disk-health-checker.min-free-space-per-disk-watermark-high-mb,yarn.nodemanager.local-dirs,"The minimum space in megabytes that must be available on a bad disk for it to be marked as good. This value should not be less than yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb. If it is less than yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb, or it is not set, it will be set to the same value as yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb. This applies to yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs.","List of directories to store localized files in. An application's localized file directory will be found in: ${yarn.nodemanager.local-dirs}/usercache/${user}/appcache/application_${appid}. Individual containers' work directories, called container_${contid}, will be subdirectories of this."
55,yarn.nodemanager.disk-health-checker.min-free-space-per-disk-watermark-high-mb,yarn.nodemanager.log-dirs,"The minimum space in megabytes that must be available on a bad disk for it to be marked as good. This value should not be less than yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb. If it is less than yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb, or it is not set, it will be set to the same value as yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb. This applies to yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs.","Where to store container logs. An application's localized log directory will be found in ${yarn.nodemanager.log-dirs}/application_${appid}. Individual containers' log directories will be below this, in directories named container_{$contid}. Each container directory will contain the files stderr, stdin, and syslog generated by that container."
56,yarn.nodemanager.disk-health-checker.min-free-space-per-disk-watermark-high-mb,yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb,"The minimum space in megabytes that must be available on a bad disk for it to be marked as good. This value should not be less than yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb. If it is less than yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb, or it is not set, it will be set to the same value as yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb. This applies to yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs.","The minimum space in megabytes that must be available on a disk for it to be used. If space on a disk falls below this threshold, it will be marked as bad. This applies to yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs when yarn.nodemanager.disk-health-checker.disk-free-space-threshold.enabled is true."
57,yarn.nodemanager.linux-container-executor.cgroups.hierarchy,yarn.nodemanager.linux-container-executor.cgroups.mount,"The cgroups hierarchy under which to place YARN proccesses (cannot contain commas). If yarn.nodemanager.linux-container-executor.cgroups.mount is false (that is, if cgroups have been pre-configured) and the YARN user has write access to the parent directory, then the directory will be created. If the directory already exists, the administrator has to give YARN write permissions to it recursively. This property only applies when the LCE resources handler is set to CgroupsLCEResourcesHandler.",Whether the LCE should attempt to mount cgroups if not found. This property only applies when the LCE resources handler is set to CgroupsLCEResourcesHandler.
58,yarn.nodemanager.linux-container-executor.cgroups.mount-path,yarn.nodemanager.linux-container-executor.cgroups.mount,"This property sets the path from which YARN will read the CGroups configuration. YARN has built-in functionality to discover the system CGroup mount paths, so use this property only if YARN's automatic mount path discovery does not work. The path specified by this property must exist before the NodeManager is launched. If yarn.nodemanager.linux-container-executor.cgroups.mount is set to true, YARN will first try to mount the CGroups at the specified path before reading them. If yarn.nodemanager.linux-container-executor.cgroups.mount is set to false, YARN will read the CGroups at the specified path. If this property is empty, YARN tries to detect the CGroups location. Please refer to NodeManagerCgroups.html in the documentation for further details. This property only applies when the LCE resources handler is set to CgroupsLCEResourcesHandler.",Whether the LCE should attempt to mount cgroups if not found. This property only applies when the LCE resources handler is set to CgroupsLCEResourcesHandler.
59,yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users,yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user,"This determines which of the two modes that LCE should use on a non-secure cluster. If this value is set to true, then all containers will be launched as the user specified in yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user. If this value is set to false, then containers will run as the user who submitted the application.",The UNIX user that containers will run as when Linux-container-executor is used in nonsecure mode (a use case for this is using cgroups) if the yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users is set to true.
60,yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user,yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users,The UNIX user that containers will run as when Linux-container-executor is used in nonsecure mode (a use case for this is using cgroups) if the yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users is set to true.,"This determines which of the two modes that LCE should use on a non-secure cluster. If this value is set to true, then all containers will be launched as the user specified in yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user. If this value is set to false, then containers will run as the user who submitted the application."
61,yarn.nodemanager.resourcemanager.connect.max-wait.ms,yarn.resourcemanager.connect.max-wait.ms,"Max time to wait for NM to connect to RM. When not set, proxy will fall back to use value of yarn.resourcemanager.connect.max-wait.ms.",Maximum time to wait to establish connection to ResourceManager.
62,yarn.nodemanager.resourcemanager.connect.retry-interval.ms,yarn.resourcemanager.connect.retry-interval.ms,"Time interval between each NM attempt to connect to RM. When not set, proxy will fall back to use value of yarn.resourcemanager.connect.retry-interval.ms.",How often to try connecting to the ResourceManager.
63,yarn.client.max-cached-nodemanagers-proxies,yarn.client.nodemanager-client-async.thread-pool-max-size,Maximum number of proxy connections to cache for node managers. If set to a value greater than zero then the cache is enabled and the NMClient and MRAppMaster will cache the specified number of node manager proxies. There will be at max one proxy per node manager. Ex. configuring it to a value of 5 will make sure that client will at max have 5 proxies cached with 5 different node managers. These connections for these proxies will be timed out if idle for more than the system wide idle timeout period. Note that this could cause issues on large clusters as many connections could linger simultaneously and lead to a large number of connection threads. The token used for authentication will be used only at connection creation time. If a new token is received then the earlier connection should be closed in order to use the new token. This and (yarn.client.nodemanager-client-async.thread-pool-max-size) are related and should be in sync (no need for them to be equal). If the value of this property is zero then the connection cache is disabled and connections will use a zero idle timeout to prevent too many connection threads on large clusters.,Max number of threads in NMClientAsync to process container management events
64,yarn.web-proxy.bind-host,yarn.web-proxy.address,"The actual address the web proxy will bind to. If this optional address is set, it overrides only the hostname portion of yarn.web-proxy.address. This is useful for making the web proxy server listen on all interfaces by setting it to 0.0.0.0","The address for the web proxy as HOST:PORT, if this is not given then the proxy will run as part of the RM"
65,yarn.timeline-service.version,yarn.timeline-service.enabled,"Indicate what is the current version of the running timeline service. For example, if ""yarn.timeline-service.version"" is 1.5, and ""yarn.timeline-service.enabled"" is true, it means the cluster will and should bring up the timeline service v.1.5 (and nothing else). On the client side, if the client uses the same version of timeline service, it should succeed. If the client chooses to use a smaller version in spite of this, then depending on how robust the compatibility story is between versions, the results may vary.","In the server side it indicates whether timeline service is enabled or not. And in the client side, users can enable it to indicate whether client wants to use timeline service. If its enabled in the client side along with security, then yarn client tries to fetch the delegation tokens for the timeline server."
66,yarn.timeline-service.bind-host,yarn.timeline-service.address,"The actual address the server will bind to. If this optional address is set, the RPC and webapp servers will bind to this address and the port specified in yarn.timeline-service.address and yarn.timeline-service.webapp.address, respectively. This is most useful for making the service listen to all interfaces by setting to 0.0.0.0.",This is default address for the timeline server to start the RPC server.
67,yarn.timeline-service.bind-host,yarn.timeline-service.webapp.address,"The actual address the server will bind to. If this optional address is set, the RPC and webapp servers will bind to this address and the port specified in yarn.timeline-service.address and yarn.timeline-service.webapp.address, respectively. This is most useful for making the service listen to all interfaces by setting to 0.0.0.0.",The http address of the timeline service web application.
68,yarn.timeline-service.recovery.enabled,yarn.timeline-service.state-store-class,"Enable timeline server to recover state after starting. If true, then yarn.timeline-service.state-store-class must be specified.",Store class name for timeline state store.
69,yarn.timeline-service.entity-group-fs-store.group-id-plugin-classpath,yarn.timeline-service.entity-group-fs-store.group-id-plugin-classes,Classpath for all plugins defined in yarn.timeline-service.entity-group-fs-store.group-id-plugin-classes.,"Plugins that can translate a timeline entity read request into a list of timeline entity group ids, separated by commas."
70,yarn.minicluster.yarn.nodemanager.resource.memory-mb,yarn.nodemanager.resource.memory-mb,As yarn.nodemanager.resource.memory-mb property but for the NodeManager in a MiniYARNCluster.,"Amount of physical memory, in MB, that can be allocated for containers. If set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true, it is automatically calculated(in case of Windows and Linux). In other cases, the default is 8192MB."
71,yarn.nodemanager.node-labels.provider,yarn.node-labels.configuration-type,"When ""yarn.node-labels.configuration-type"" is configured with ""distributed"" in RM, Administrators can configure in NM the provider for the node labels by configuring this parameter. Administrators can configure ""config"", ""script"" or the class name of the provider. Configured class needs to extend org.apache.hadoop.yarn.server.nodemanager.nodelabels.NodeLabelsProvider. If ""config"" is configured, then ""ConfigurationNodeLabelsProvider"" and if ""script"" is configured, then ""ScriptNodeLabelsProvider"" will be used.","Set configuration type for node labels. Administrators can specify ""centralized"", ""delegated-centralized"" or ""distributed""."
72,yarn.nodemanager.node-labels.provider.fetch-interval-ms,yarn.nodemanager.node-labels.provider,"When ""yarn.nodemanager.node-labels.provider"" is configured with ""config"", ""Script"" or the configured class extends AbstractNodeLabelsProvider, then periodically node labels are retrieved from the node labels provider. This configuration is to define the interval period. If -1 is configured then node labels are retrieved from provider only during initialization. Defaults to 10 mins.","When ""yarn.node-labels.configuration-type"" is configured with ""distributed"" in RM, Administrators can configure in NM the provider for the node labels by configuring this parameter. Administrators can configure ""config"", ""script"" or the class name of the provider. Configured class needs to extend org.apache.hadoop.yarn.server.nodemanager.nodelabels.NodeLabelsProvider. If ""config"" is configured, then ""ConfigurationNodeLabelsProvider"" and if ""script"" is configured, then ""ScriptNodeLabelsProvider"" will be used."
73,yarn.nodemanager.node-labels.provider.configured-node-partition,yarn.nodemanager.node-labels.provider,"When ""yarn.nodemanager.node-labels.provider"" is configured with ""config"" then ConfigurationNodeLabelsProvider fetches the partition label from this parameter.","When ""yarn.node-labels.configuration-type"" is configured with ""distributed"" in RM, Administrators can configure in NM the provider for the node labels by configuring this parameter. Administrators can configure ""config"", ""script"" or the class name of the provider. Configured class needs to extend org.apache.hadoop.yarn.server.nodemanager.nodelabels.NodeLabelsProvider. If ""config"" is configured, then ""ConfigurationNodeLabelsProvider"" and if ""script"" is configured, then ""ScriptNodeLabelsProvider"" will be used."
74,yarn.nodemanager.node-labels.provider.fetch-timeout-ms,yarn.nodemanager.node-labels.provider,"When ""yarn.nodemanager.node-labels.provider"" is configured with ""Script"" then this configuration provides the timeout period after which it will interrupt the script which queries the Node labels. Defaults to 20 mins.","When ""yarn.node-labels.configuration-type"" is configured with ""distributed"" in RM, Administrators can configure in NM the provider for the node labels by configuring this parameter. Administrators can configure ""config"", ""script"" or the class name of the provider. Configured class needs to extend org.apache.hadoop.yarn.server.nodemanager.nodelabels.NodeLabelsProvider. If ""config"" is configured, then ""ConfigurationNodeLabelsProvider"" and if ""script"" is configured, then ""ScriptNodeLabelsProvider"" will be used."
75,yarn.resourcemanager.node-labels.provider,yarn.node-labels.configuration-type,"When node labels ""yarn.node-labels.configuration-type"" is of type ""delegated-centralized"", administrators should configure the class for fetching node labels by ResourceManager. Configured class needs to extend org.apache.hadoop.yarn.server.resourcemanager.nodelabels. RMNodeLabelsMappingProvider.","Set configuration type for node labels. Administrators can specify ""centralized"", ""delegated-centralized"" or ""distributed""."
76,yarn.resourcemanager.node-labels.provider.fetch-interval-ms,yarn.node-labels.configuration-type,"When ""yarn.node-labels.configuration-type"" is configured with ""delegated-centralized"", then periodically node labels are retrieved from the node labels provider. This configuration is to define the interval. If -1 is configured then node labels are retrieved from provider only once for each node after it registers. Defaults to 30 mins.","Set configuration type for node labels. Administrators can specify ""centralized"", ""delegated-centralized"" or ""distributed""."
77,yarn.nodemanager.node-attributes.provider.configured-node-attributes,yarn.nodemanager.node-attributes.provider,"When ""yarn.nodemanager.node-attributes.provider"" is configured with ""config"" then ConfigurationNodeAttributesProvider fetches node attributes from this parameter.","This property determines which provider will be plugged by the node manager to collect node-attributes. Administrators can configure ""config"", ""script"" or the class name of the provider. Configured class needs to extend org.apache.hadoop.yarn.server.nodemanager.nodelabels.NodeAttributesProvider. If ""config"" is configured, then ""ConfigurationNodeLabelsProvider"" and if ""script"" is configured, then ""ScriptBasedNodeAttributesProvider"" will be used."
78,yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds,yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds.min,"Defines how often NMs wake up to upload log files. The default value is -1. By default, the logs will be uploaded when the application is finished. By setting this configuration logs can be uploaded periodically while the application is running. The minimum positive accepted value can be configured by the setting ""yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds.min"".","Defines the positive minimum hard limit for ""yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds"". If this configuration has been set less than its default value (3600) the NodeManager may raise a warning."
79,yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds.min,yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds,"Defines the positive minimum hard limit for ""yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds"". If this configuration has been set less than its default value (3600) the NodeManager may raise a warning.","Defines how often NMs wake up to upload log files. The default value is -1. By default, the logs will be uploaded when the application is finished. By setting this configuration logs can be uploaded periodically while the application is running. The minimum positive accepted value can be configured by the setting ""yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds.min""."
80,yarn.app.attempt.diagnostics.limit.kc,yarn.resourcemanager.state-store.max-completed-applications,"Defines the limit of the diagnostics message of an application attempt, in kilo characters (character count * 1024). When using ZooKeeper to store application state behavior, its important to limit the size of the diagnostic messages to prevent YARN from overwhelming ZooKeeper. In cases where yarn.resourcemanager.state-store.max-completed-applications is set to a large number, it may be desirable to reduce the value of this property to limit the total data stored.","The maximum number of completed applications RM state store keeps, less than or equals to ${yarn.resourcemanager.max-completed-applications}. By default, it equals to ${yarn.resourcemanager.max-completed-applications}. This ensures that the applications kept in the state store are consistent with the applications remembered in RM memory. Any values larger than ${yarn.resourcemanager.max-completed-applications} will be reset to ${yarn.resourcemanager.max-completed-applications}. Note that this value impacts the RM recovery performance. Typically, a smaller value indicates better performance on RM recovery."
81,yarn.router.bind-host,yarn.router.webapp.address,"The actual address the server will bind to. If this optional address is set, the RPC and webapp servers will bind to this address and the port specified in yarn.router.address and yarn.router.webapp.address, respectively. This is most useful for making Router listen to all interfaces by setting to 0.0.0.0.","The http address of the Router web application. If only a host is provided as the value, the webapp will be served on a random port."
82,yarn.timeline-service.entity-group-fs-store.with-user-dir,yarn.timeline-service.entity-group-fs-store.active-dir,It is TimelineClient 1.5 configuration whether to store active application timeline data with in user directory i.e ${yarn.timeline-service.entity-group-fs-store.active-dir}/${user.name},HDFS path to store active application timeline data
83,yarn.scheduler.configuration.leveldb-store.path,yarn.scheduler.configuration.store.class,"The storage path for LevelDB implementation of configuration store, when yarn.scheduler.configuration.store.class is configured to be ""leveldb"".","The type of configuration store to use for scheduler configurations. Default is ""file"", which uses file based capacity-scheduler.xml to retrieve and change scheduler configuration. To enable API based scheduler configuration, use either ""memory"" (in memory storage, no persistence across restarts), ""leveldb"" (leveldb based storage), or ""zk"" (zookeeper based storage). API based configuration is only useful when using a scheduler which supports mutable configuration. Currently only capacity scheduler supports this."
84,yarn.scheduler.configuration.leveldb-store.compaction-interval-secs,yarn.scheduler.configuration.store.class,"The compaction interval for LevelDB configuration store in secs, when yarn.scheduler.configuration.store.class is configured to be ""leveldb"". Default is one day.","The type of configuration store to use for scheduler configurations. Default is ""file"", which uses file based capacity-scheduler.xml to retrieve and change scheduler configuration. To enable API based scheduler configuration, use either ""memory"" (in memory storage, no persistence across restarts), ""leveldb"" (leveldb based storage), or ""zk"" (zookeeper based storage). API based configuration is only useful when using a scheduler which supports mutable configuration. Currently only capacity scheduler supports this."
85,yarn.scheduler.configuration.store.max-logs,yarn.scheduler.configuration.store.class,"The max number of configuration change log entries kept in config store, when yarn.scheduler.configuration.store.class is configured to be ""leveldb"" or ""zk"". Default is 1000 for either.","The type of configuration store to use for scheduler configurations. Default is ""file"", which uses file based capacity-scheduler.xml to retrieve and change scheduler configuration. To enable API based scheduler configuration, use either ""memory"" (in memory storage, no persistence across restarts), ""leveldb"" (leveldb based storage), or ""zk"" (zookeeper based storage). API based configuration is only useful when using a scheduler which supports mutable configuration. Currently only capacity scheduler supports this."
86,yarn.timeline-service.reader.bind-host,yarn.timeline-service.reader.webapp.address,"The actual address timeline reader will bind to. If this optional address is set, the reader server will bind to this address and the port specified in yarn.timeline-service.reader.webapp.address. This is most useful for making the service listen to all interfaces by setting to 0.0.0.0.",The http address of the timeline reader web application.
87,yarn.nodemanager.numa-awareness.read-topology,yarn.nodemanager.numa-awareness.node-ids,"Whether to read the NUMA topology from the system or from the configurations. If the value is true then NM reads the NUMA topology from system using the command 'numactl --hardware'. If the value is false then NM reads the topology from the configurations 'yarn.nodemanager.numa-awareness.node-ids'(for node id's), 'yarn.nodemanager.numa-awareness.<NODE_ID>.memory'(for each node memory), 'yarn.nodemanager.numa-awareness.<NODE_ID>.cpus'(for each node cpus).","NUMA node id's in the form of comma separated list. Memory and No of CPUs will be read using the properties 'yarn.nodemanager.numa-awareness.<NODE_ID>.memory' and 'yarn.nodemanager.numa-awareness.<NODE_ID>.cpus' for each id specified in this value. This property value will be read only when 'yarn.nodemanager.numa-awareness.read-topology=false'. For example, if yarn.nodemanager.numa-awareness.node-ids=0,1 then need to specify memory and cpus for node id's '0' and '1' like below, yarn.nodemanager.numa-awareness.0.memory=73717 yarn.nodemanager.numa-awareness.0.cpus=4 yarn.nodemanager.numa-awareness.1.memory=73727 yarn.nodemanager.numa-awareness.1.cpus=4"
88,yarn.nodemanager.numa-awareness.node-ids,yarn.nodemanager.numa-awareness.read-topology,"NUMA node id's in the form of comma separated list. Memory and No of CPUs will be read using the properties 'yarn.nodemanager.numa-awareness.<NODE_ID>.memory' and 'yarn.nodemanager.numa-awareness.<NODE_ID>.cpus' for each id specified in this value. This property value will be read only when 'yarn.nodemanager.numa-awareness.read-topology=false'. For example, if yarn.nodemanager.numa-awareness.node-ids=0,1 then need to specify memory and cpus for node id's '0' and '1' like below, yarn.nodemanager.numa-awareness.0.memory=73717 yarn.nodemanager.numa-awareness.0.cpus=4 yarn.nodemanager.numa-awareness.1.memory=73727 yarn.nodemanager.numa-awareness.1.cpus=4","Whether to read the NUMA topology from the system or from the configurations. If the value is true then NM reads the NUMA topology from system using the command 'numactl --hardware'. If the value is false then NM reads the topology from the configurations 'yarn.nodemanager.numa-awareness.node-ids'(for node id's), 'yarn.nodemanager.numa-awareness.<NODE_ID>.memory'(for each node memory), 'yarn.nodemanager.numa-awareness.<NODE_ID>.cpus'(for each node cpus)."
89,yarn.nodemanager.elastic-memory-control.enabled,yarn.nodemanager.resource.memory-mb,"Enable elastic memory control. This is a Linux only feature. When enabled, the node manager adds a listener to receive an event, if all the containers exceeded a limit. The limit is specified by yarn.nodemanager.resource.memory-mb. If this is not set, the limit is set based on the capabilities. See yarn.nodemanager.resource.detect-hardware-capabilities for details. The limit applies to the physical or virtual (rss+swap) memory depending on whether yarn.nodemanager.pmem-check-enabled or yarn.nodemanager.vmem-check-enabled is set.","Amount of physical memory, in MB, that can be allocated for containers. If set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true, it is automatically calculated(in case of Windows and Linux). In other cases, the default is 8192MB."
90,yarn.nodemanager.elastic-memory-control.enabled,yarn.nodemanager.pmem-check-enabled,"Enable elastic memory control. This is a Linux only feature. When enabled, the node manager adds a listener to receive an event, if all the containers exceeded a limit. The limit is specified by yarn.nodemanager.resource.memory-mb. If this is not set, the limit is set based on the capabilities. See yarn.nodemanager.resource.detect-hardware-capabilities for details. The limit applies to the physical or virtual (rss+swap) memory depending on whether yarn.nodemanager.pmem-check-enabled or yarn.nodemanager.vmem-check-enabled is set.",Whether physical memory limits will be enforced for containers.
91,yarn.nodemanager.elastic-memory-control.enabled,yarn.nodemanager.vmem-check-enabled,"Enable elastic memory control. This is a Linux only feature. When enabled, the node manager adds a listener to receive an event, if all the containers exceeded a limit. The limit is specified by yarn.nodemanager.resource.memory-mb. If this is not set, the limit is set based on the capabilities. See yarn.nodemanager.resource.detect-hardware-capabilities for details. The limit applies to the physical or virtual (rss+swap) memory depending on whether yarn.nodemanager.pmem-check-enabled or yarn.nodemanager.vmem-check-enabled is set.",Whether virtual memory limits will be enforced for containers.
92,yarn.nodemanager.elastic-memory-control.enabled,yarn.nodemanager.resource.detect-hardware-capabilities,"Enable elastic memory control. This is a Linux only feature. When enabled, the node manager adds a listener to receive an event, if all the containers exceeded a limit. The limit is specified by yarn.nodemanager.resource.memory-mb. If this is not set, the limit is set based on the capabilities. See yarn.nodemanager.resource.detect-hardware-capabilities for details. The limit applies to the physical or virtual (rss+swap) memory depending on whether yarn.nodemanager.pmem-check-enabled or yarn.nodemanager.vmem-check-enabled is set.",Enable auto-detection of node capabilities such as memory and CPU.
93,yarn.nodemanager.elastic-memory-control.oom-handler,yarn.nodemanager.elastic-memory-control.enabled,"The name of a JVM class. The class must implement the Runnable interface. It is called, if yarn.nodemanager.elastic-memory-control.enabled is set and the system reaches its memory limit. When called the handler must preempt a container, since all containers are frozen by cgroups. Once preempted some memory is released, so that the kernel can resume all containers. Because of this the handler has to act quickly.","Enable elastic memory control. This is a Linux only feature. When enabled, the node manager adds a listener to receive an event, if all the containers exceeded a limit. The limit is specified by yarn.nodemanager.resource.memory-mb. If this is not set, the limit is set based on the capabilities. See yarn.nodemanager.resource.detect-hardware-capabilities for details. The limit applies to the physical or virtual (rss+swap) memory depending on whether yarn.nodemanager.pmem-check-enabled or yarn.nodemanager.vmem-check-enabled is set."
94,yarn.resourcemanager.am.global.max-attempts,yarn.resourcemanager.am.max-attempts,"The maximum number of application attempts. It's a global setting for all application masters. Each application master can specify its individual maximum number of application attempts via the API, but the individual number cannot be more than the global upper bound. If it is, the resourcemanager will override it. The default number value is set to yarn.resourcemanager.am.max-attempts.","The default maximum number of application attempts, if unset by the user. Each application master can specify its individual maximum number of application attempts via the API, but the individual number cannot be more than the global upper bound in yarn.resourcemanager.am.global.max-attempts. The default number is set to 2, to allow at least one retry for AM."
