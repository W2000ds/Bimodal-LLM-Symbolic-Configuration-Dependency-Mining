ID,Option A,Option B,Option A Description,Option B Description
1,hadoop.security.dns.nameserver,hadoop.security.dns.interface, The host name or IP address of the name server (DNS) which a service Node should use to determine its own host name for Kerberos Login. Requires hadoop.security.dns.interface. Most clusters will not require this setting.  ," The name of the Network Interface from which the service should determine its host name for Kerberos login. e.g. eth2. In a multi-homed environment, the setting can be used to affect the _HOST substitution in the service Kerberos principal. If this configuration value is not set, the service will use its default hostname as returned by InetAddress.getLocalHost().getCanonicalHostName(). Most clusters will not require this setting.  "
2,hadoop.security.groups.cache.background.reload,hadoop.security.groups.cache.background.reload.threads," Whether to reload expired user->group mappings using a background thread pool. If set to true, a pool of hadoop.security.groups.cache.background.reload.threads is created to update the cache in the background.  ", Only relevant if hadoop.security.groups.cache.background.reload is true. Controls the number of concurrent background user->group cache entry refreshes. Pending refresh requests beyond this value are queued and processed when a thread is free.  
3,hadoop.security.groups.cache.background.reload.threads,hadoop.security.groups.cache.background.reload, Only relevant if hadoop.security.groups.cache.background.reload is true. Controls the number of concurrent background user->group cache entry refreshes. Pending refresh requests beyond this value are queued and processed when a thread is free.  ," Whether to reload expired user->group mappings using a background thread pool. If set to true, a pool of hadoop.security.groups.cache.background.reload.threads is created to update the cache in the background.  "
4,hadoop.security.group.mapping.ldap.ssl.keystore.password.file,hadoop.security.group.mapping," The path to a file containing the password of the LDAP SSL keystore. If the password is not configured in credential providers and the property hadoop.security.group.mapping.ldap.ssl.keystore.password is not set, LDAPGroupsMapping reads password from the file. IMPORTANT: This file should be readable only by the Unix user running the daemons and should be a local file.  "," Class for user to group mapping (get groups for a given user) for ACL. The default implementation, org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, will determine if the Java Native Interface (JNI) is available. If JNI is available the implementation will use the API within hadoop to resolve a list of groups for a user. If JNI is not available then the shell implementation, ShellBasedUnixGroupsMapping, is used.  This implementation shells out to the Linux/Unix environment with the "
5,hadoop.security.group.mapping.ldap.ssl.keystore.password.file,hadoop.security.group.mapping.ldap.ssl," The path to a file containing the password of the LDAP SSL keystore. If the password is not configured in credential providers and the property hadoop.security.group.mapping.ldap.ssl.keystore.password is not set, LDAPGroupsMapping reads password from the file. IMPORTANT: This file should be readable only by the Unix user running the daemons and should be a local file.  ", Whether or not to use SSL when connecting to the LDAP server.  
6,hadoop.security.group.mapping.ldap.ssl.keystore.password.file,hadoop.security.group.mapping.ldap.ssl.keystore," The path to a file containing the password of the LDAP SSL keystore. If the password is not configured in credential providers and the property hadoop.security.group.mapping.ldap.ssl.keystore.password is not set, LDAPGroupsMapping reads password from the file. IMPORTANT: This file should be readable only by the Unix user running the daemons and should be a local file.  ", File path to the SSL keystore that contains the SSL certificate required by the LDAP server.  
7,hadoop.security.group.mapping.ldap.ssl.keystore.password.file,hadoop.security.group.mapping.ldap.ssl.keystore.password," The path to a file containing the password of the LDAP SSL keystore. If the password is not configured in credential providers and the property hadoop.security.group.mapping.ldap.ssl.keystore.password is not set, LDAPGroupsMapping reads password from the file. IMPORTANT: This file should be readable only by the Unix user running the daemons and should be a local file.  ", The password of the LDAP SSL keystore. this property name is used as an alias to get the password from credential providers. If the password can not be found and hadoop.security.credential.clear-text-fallback is true LDAPGroupsMapping uses the value of this property for password.  
8,hadoop.security.group.mapping.ldap.ssl.keystore.password,hadoop.security.credential.clear-text-fallback, The password of the LDAP SSL keystore. this property name is used as an alias to get the password from credential providers. If the password can not be found and hadoop.security.credential.clear-text-fallback is true LDAPGroupsMapping uses the value of this property for password.  , true or false to indicate whether or not to fall back to storing credential password as clear text. The default value is true. This property only works when the password can't not be found from credential providers.  
9,hadoop.security.group.mapping.ldap.bind.password.file,hadoop.security.group.mapping," The path to a file containing the password of the bind user. If the password is not configured in credential providers and the property hadoop.security.group.mapping.ldap.bind.password is not set, LDAPGroupsMapping reads password from the file. IMPORTANT: This file should be readable only by the Unix user running the daemons and should be a local file.  "," Class for user to group mapping (get groups for a given user) for ACL. The default implementation, org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, will determine if the Java Native Interface (JNI) is available. If JNI is available the implementation will use the API within hadoop to resolve a list of groups for a user. If JNI is not available then the shell implementation, ShellBasedUnixGroupsMapping, is used.  This implementation shells out to the Linux/Unix environment with the "
10,hadoop.security.group.mapping.ldap.bind.password.file,hadoop.security.group.mapping.ldap.bind.password," The path to a file containing the password of the bind user. If the password is not configured in credential providers and the property hadoop.security.group.mapping.ldap.bind.password is not set, LDAPGroupsMapping reads password from the file. IMPORTANT: This file should be readable only by the Unix user running the daemons and should be a local file.  ", The password of the bind user. this property name is used as an alias to get the password from credential providers. If the password can not be found and hadoop.security.credential.clear-text-fallback is true LDAPGroupsMapping uses the value of this property for password.  
11,hadoop.security.group.mapping.ldap.bind.password,hadoop.security.credential.clear-text-fallback, The password of the bind user. this property name is used as an alias to get the password from credential providers. If the password can not be found and hadoop.security.credential.clear-text-fallback is true LDAPGroupsMapping uses the value of this property for password.  , true or false to indicate whether or not to fall back to storing credential password as clear text. The default value is true. This property only works when the password can't not be found from credential providers.  
12,hadoop.security.group.mapping.ldap.userbase,hadoop.security.group.mapping," The search base for the LDAP connection for user search query. This is a distinguished name, and its the root of the LDAP directory for users. If not set, hadoop.security.group.mapping.ldap.base is used.  "," Class for user to group mapping (get groups for a given user) for ACL. The default implementation, org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, will determine if the Java Native Interface (JNI) is available. If JNI is available the implementation will use the API within hadoop to resolve a list of groups for a user. If JNI is not available then the shell implementation, ShellBasedUnixGroupsMapping, is used.  This implementation shells out to the Linux/Unix environment with the "
13,hadoop.security.group.mapping.ldap.userbase,hadoop.security.group.mapping.ldap.base," The search base for the LDAP connection for user search query. This is a distinguished name, and its the root of the LDAP directory for users. If not set, hadoop.security.group.mapping.ldap.base is used.  "," The search base for the LDAP connection. This is a distinguished name, and will typically be the root of the LDAP directory.  "
14,hadoop.security.group.mapping.ldap.groupbase,hadoop.security.group.mapping," The search base for the LDAP connection for group search . This is a distinguished name, and its the root of the LDAP directory for groups. If not set, hadoop.security.group.mapping.ldap.base is used.  "," Class for user to group mapping (get groups for a given user) for ACL. The default implementation, org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, will determine if the Java Native Interface (JNI) is available. If JNI is available the implementation will use the API within hadoop to resolve a list of groups for a user. If JNI is not available then the shell implementation, ShellBasedUnixGroupsMapping, is used.  This implementation shells out to the Linux/Unix environment with the "
15,hadoop.security.group.mapping.ldap.groupbase,hadoop.security.group.mapping.ldap.base," The search base for the LDAP connection for group search . This is a distinguished name, and its the root of the LDAP directory for groups. If not set, hadoop.security.group.mapping.ldap.base is used.  "," The search base for the LDAP connection. This is a distinguished name, and will typically be the root of the LDAP directory.  "
16,hadoop.security.group.mapping.ldap.search.filter.user,hadoop.security.group.mapping," An additional filter to use when searching for LDAP users. The default will usually be appropriate for Active Directory installations. If connecting to an LDAP server with a non-AD schema, this should be replaced with (&(objectClass=inetOrgPerson)(uid={0}). {0} is a special string used to denote where the username fits into the filter. If the LDAP server supports posixGroups, Hadoop can enable the feature by setting the value of this property to ""posixAccount"" and the value of the hadoop.security.group.mapping.ldap.search.filter.group property to ""posixGroup"".  "," Class for user to group mapping (get groups for a given user) for ACL. The default implementation, org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, will determine if the Java Native Interface (JNI) is available. If JNI is available the implementation will use the API within hadoop to resolve a list of groups for a user. If JNI is not available then the shell implementation, ShellBasedUnixGroupsMapping, is used.  This implementation shells out to the Linux/Unix environment with the "
17,hadoop.security.group.mapping.ldap.search.filter.user,hadoop.security.group.mapping.ldap.search.filter.group," An additional filter to use when searching for LDAP users. The default will usually be appropriate for Active Directory installations. If connecting to an LDAP server with a non-AD schema, this should be replaced with (&(objectClass=inetOrgPerson)(uid={0}). {0} is a special string used to denote where the username fits into the filter. If the LDAP server supports posixGroups, Hadoop can enable the feature by setting the value of this property to ""posixAccount"" and the value of the hadoop.security.group.mapping.ldap.search.filter.group property to ""posixGroup"".  ", An additional filter to use when searching for LDAP groups. This should be changed when resolving groups against a non-Active Directory installation. See the description of hadoop.security.group.mapping.ldap.search.filter.user to enable posixGroups support.  
18,hadoop.security.group.mapping.ldap.search.filter.group,hadoop.security.group.mapping, An additional filter to use when searching for LDAP groups. This should be changed when resolving groups against a non-Active Directory installation. See the description of hadoop.security.group.mapping.ldap.search.filter.user to enable posixGroups support.  ," Class for user to group mapping (get groups for a given user) for ACL. The default implementation, org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, will determine if the Java Native Interface (JNI) is available. If JNI is available the implementation will use the API within hadoop to resolve a list of groups for a user. If JNI is not available then the shell implementation, ShellBasedUnixGroupsMapping, is used.  This implementation shells out to the Linux/Unix environment with the "
19,hadoop.security.group.mapping.ldap.search.filter.group,hadoop.security.group.mapping.ldap.search.filter.user, An additional filter to use when searching for LDAP groups. This should be changed when resolving groups against a non-Active Directory installation. See the description of hadoop.security.group.mapping.ldap.search.filter.user to enable posixGroups support.  ," An additional filter to use when searching for LDAP users. The default will usually be appropriate for Active Directory installations. If connecting to an LDAP server with a non-AD schema, this should be replaced with (&(objectClass=inetOrgPerson)(uid={0}). {0} is a special string used to denote where the username fits into the filter. If the LDAP server supports posixGroups, Hadoop can enable the feature by setting the value of this property to ""posixAccount"" and the value of the hadoop.security.group.mapping.ldap.search.filter.group property to ""posixGroup"".  "
20,hadoop.security.group.mapping.ldap.search.group.hierarchy.levels,hadoop.security.group.mapping, The number of levels to go up the group hierarchy when determining which groups a user is part of. 0 Will represent checking just the group that the user belongs to.  Each additional level will raise the time it takes to execute a query by at most hadoop.security.group.mapping.ldap.directory.search.timeout. The default will usually be appropriate for all LDAP systems.  ," Class for user to group mapping (get groups for a given user) for ACL. The default implementation, org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, will determine if the Java Native Interface (JNI) is available. If JNI is available the implementation will use the API within hadoop to resolve a list of groups for a user. If JNI is not available then the shell implementation, ShellBasedUnixGroupsMapping, is used.  This implementation shells out to the Linux/Unix environment with the "
21,hadoop.security.group.mapping.ldap.search.group.hierarchy.levels,hadoop.security.group.mapping.ldap.directory.search.timeout, The number of levels to go up the group hierarchy when determining which groups a user is part of. 0 Will represent checking just the group that the user belongs to.  Each additional level will raise the time it takes to execute a query by at most hadoop.security.group.mapping.ldap.directory.search.timeout. The default will usually be appropriate for all LDAP systems.  , The attribute applied to the LDAP SearchControl properties to set a maximum time limit when searching and awaiting a result. Set to 0 if infinite wait period is desired. Default is 10 seconds. Units in milliseconds.  
22,hadoop.rpc.protection,hadoop.security.saslproperties.resolver.class,"A comma-separated list of protection values for secured sasl   connections. Possible values are authentication, integrity and privacy.   authentication means authentication only and no integrity or privacy;   integrity implies authentication and integrity are enabled; and privacy   implies all of authentication, integrity and privacy are enabled.   hadoop.security.saslproperties.resolver.class can be used to override   the hadoop.rpc.protection for a connection at the server side.  ","SaslPropertiesResolver used to resolve the QOP used for a   connection. If not specified, the full set of values specified in   hadoop.rpc.protection is used while determining the QOP used for the   connection. If a class is specified, then the QOP values returned by   the class will be used while determining the QOP used for the connection.  "
23,hadoop.security.saslproperties.resolver.class,hadoop.rpc.protection,"SaslPropertiesResolver used to resolve the QOP used for a   connection. If not specified, the full set of values specified in   hadoop.rpc.protection is used while determining the QOP used for the   connection. If a class is specified, then the QOP values returned by   the class will be used while determining the QOP used for the connection.  ","A comma-separated list of protection values for secured sasl   connections. Possible values are authentication, integrity and privacy.   authentication means authentication only and no integrity or privacy;   integrity implies authentication and integrity are enabled; and privacy   implies all of authentication, integrity and privacy are enabled.   hadoop.security.saslproperties.resolver.class can be used to override   the hadoop.rpc.protection for a connection at the server side.  "
24,io.bytes.per.checksum,io.file.buffer.size,The number of bytes per checksum.  Must not be larger than  io.file.buffer.size.,"The size of buffer for use in sequence files.  The size of this buffer should probably be a multiple of hardware  page size (4096 on Intel x86), and it determines how much data is  buffered during read and write operations."
25,fs.default.name,fs.defaultFS,Deprecated. Use (fs.defaultFS) property  instead,"The name of the default file system.  A URI whose  scheme and authority determine the FileSystem implementation.  The  uri's scheme determines the config property (fs.SCHEME.impl) naming  the FileSystem implementation class.  The uri's authority is used to  determine the host, port, etc. for a filesystem."
26,fs.trash.checkpoint.interval,fs.trash.interval,"Number of minutes between trash checkpoints.  Should be smaller or equal to fs.trash.interval. If zero,  the value is set to the value of fs.trash.interval.  Every time the checkpointer runs it creates a new checkpoint  out of current and removes checkpoints created more than  fs.trash.interval minutes ago.  ","Number of minutes after which the checkpoint  gets deleted.  If zero, the trash feature is disabled.  This option may be configured both on the server and the  client. If trash is disabled server side then the client  side configuration is checked. If trash is enabled on the  server side then the value configured on the server is  used and the client configuration value is ignored.  "
27,fs.ftp.host.port,fs.ftp.host, FTP filesystem connects to fs.ftp.host on this port  ,FTP filesystem connects to this server
28,fs.s3n.multipart.uploads.enabled,fs.s3n.multipart.uploads.block.size,"Setting this property to true enables multiple uploads to  native S3 filesystem. When uploading a file, it is split into blocks  if the size is larger than fs.s3n.multipart.uploads.block.size.  ",The block size for multipart uploads to native S3 filesystem.  Default size is 64MB.  
29,fs.s3a.aws.credentials.provider,fs.s3a.access.key," Comma-separated class names of credential provider classes which implement com.amazonaws.auth.AWSCredentialsProvider. These are loaded and queried in sequence for a valid set of credentials. Each listed class must implement one of the following means of construction, which are attempted in order: 1. a public constructor accepting java.net.URI and  org.apache.hadoop.conf.Configuration, 2. a public static method named getInstance that accepts no    arguments and returns an instance of    com.amazonaws.auth.AWSCredentialsProvider, or 3. a public default constructor. Specifying org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider allows anonymous access to a publicly accessible S3 bucket without any credentials. Please note that allowing anonymous access to an S3 bucket compromises security and therefore is unsuitable for most use cases. It can be useful for accessing public data sets without requiring AWS credentials. If unspecified, then the default list of credential provider classes, queried in sequence, is: 1. org.apache.hadoop.fs.s3a.BasicAWSCredentialsProvider: supports static  configuration of AWS access key ID and secret access key.  See also  fs.s3a.access.key and fs.s3a.secret.key. 2. com.amazonaws.auth.EnvironmentVariableCredentialsProvider: supports  configuration of AWS access key ID and secret access key in  environment variables named AWS_ACCESS_KEY_ID and  AWS_SECRET_ACCESS_KEY, as documented in the AWS SDK. 3. org.apache.hadoop.fs.s3a.SharedInstanceProfileCredentialsProvider:  a shared instance of  com.amazonaws.auth.InstanceProfileCredentialsProvider from the AWS  SDK, which supports use of instance profile credentials if running  in an EC2 VM.  Using this shared instance potentially reduces load  on the EC2 instance metadata service for multi-threaded  applications.  ",AWS access key ID used by S3A file system. Omit for IAM role-based or provider-based authentication.
30,fs.s3a.aws.credentials.provider,fs.s3a.secret.key," Comma-separated class names of credential provider classes which implement com.amazonaws.auth.AWSCredentialsProvider. These are loaded and queried in sequence for a valid set of credentials. Each listed class must implement one of the following means of construction, which are attempted in order: 1. a public constructor accepting java.net.URI and  org.apache.hadoop.conf.Configuration, 2. a public static method named getInstance that accepts no    arguments and returns an instance of    com.amazonaws.auth.AWSCredentialsProvider, or 3. a public default constructor. Specifying org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider allows anonymous access to a publicly accessible S3 bucket without any credentials. Please note that allowing anonymous access to an S3 bucket compromises security and therefore is unsuitable for most use cases. It can be useful for accessing public data sets without requiring AWS credentials. If unspecified, then the default list of credential provider classes, queried in sequence, is: 1. org.apache.hadoop.fs.s3a.BasicAWSCredentialsProvider: supports static  configuration of AWS access key ID and secret access key.  See also  fs.s3a.access.key and fs.s3a.secret.key. 2. com.amazonaws.auth.EnvironmentVariableCredentialsProvider: supports  configuration of AWS access key ID and secret access key in  environment variables named AWS_ACCESS_KEY_ID and  AWS_SECRET_ACCESS_KEY, as documented in the AWS SDK. 3. org.apache.hadoop.fs.s3a.SharedInstanceProfileCredentialsProvider:  a shared instance of  com.amazonaws.auth.InstanceProfileCredentialsProvider from the AWS  SDK, which supports use of instance profile credentials if running  in an EC2 VM.  Using this shared instance potentially reduces load  on the EC2 instance metadata service for multi-threaded  applications.  ",AWS secret key used by S3A file system. Omit for IAM role-based or provider-based authentication.
31,fs.s3a.security.credential.provider.path,hadoop.security.credential.provider.path," Optional comma separated list of credential providers, a list which is prepended to that set in hadoop.security.credential.provider.path  ", A comma-separated list of URLs that indicates the type and location of a list of providers that should be consulted.  
32,fs.s3a.proxy.port,fs.s3a.connection.ssl.enabled,"Proxy server port. If this property is not set but fs.s3a.proxy.host is, port 80 or 443 is assumed (consistent with the value of fs.s3a.connection.ssl.enabled).",Enables or disables SSL connections to S3.
33,fs.s3a.proxy.port,fs.s3a.proxy.host,"Proxy server port. If this property is not set but fs.s3a.proxy.host is, port 80 or 443 is assumed (consistent with the value of fs.s3a.connection.ssl.enabled).",Hostname of the (optional) proxy server for S3 connections.
34,fs.s3a.multipart.purge,fs.s3a.multipart.purge.age,"True if you want to purge existing multipart uploads that may not have been completed/aborted correctly. The corresponding purge age is defined in fs.s3a.multipart.purge.age. If set, when the filesystem is instantiated then all outstanding uploads older than the purge age will be terminated -across the entire bucket. This will impact multipart uploads by other applications and users. so should be used sparingly, with an age value chosen to stop failed uploads, without breaking ongoing operations.  ",Minimum age in seconds of multipart uploads to purge.  
35,fs.s3a.server-side-encryption.key,fs.s3a.server-side-encryption-algorithm,"Specific encryption key to use if fs.s3a.server-side-encryption-algorithm has been set to 'SSE-KMS' or 'SSE-C'. In the case of SSE-C, the value of this property should be the Base64 encoded key. If you are using SSE-KMS and leave this property empty, you'll be using your default's S3 KMS key, otherwise you should set this property to the specific KMS key id.  ","Specify a server-side encryption algorithm for s3a: file system. Unset by default.  It supports the following values: 'AES256' (for SSE-S3), 'SSE-KMS' and 'SSE-C'.  "
36,fs.s3a.fast.upload,fs.s3a.fast.upload.buffer, Use the incremental block-based fast upload mechanism with the buffering mechanism set in fs.s3a.fast.upload.buffer.  ," The buffering mechanism to use when using S3A fast upload (fs.s3a.fast.upload=true). Values: disk, array, bytebuffer. This configuration option has no effect if fs.s3a.fast.upload is false. ""disk"" will use the directories listed in fs.s3a.buffer.dir as the location(s) to save data prior to being uploaded. ""array"" uses arrays in the JVM heap ""bytebuffer"" uses off-heap memory within the JVM. Both ""array"" and ""bytebuffer"" will consume memory in a single stream up to the number of blocks set by:  fs.s3a.multipart.size * fs.s3a.fast.upload.active.blocks. If using either of these mechanisms, keep this value low The total number of threads performing work across all threads is set by fs.s3a.threads.max, with fs.s3a.max.total.tasks values setting the number of queued work items.  "
37,fs.s3a.fast.upload.buffer,fs.s3a.threads.max," The buffering mechanism to use when using S3A fast upload (fs.s3a.fast.upload=true). Values: disk, array, bytebuffer. This configuration option has no effect if fs.s3a.fast.upload is false. ""disk"" will use the directories listed in fs.s3a.buffer.dir as the location(s) to save data prior to being uploaded. ""array"" uses arrays in the JVM heap ""bytebuffer"" uses off-heap memory within the JVM. Both ""array"" and ""bytebuffer"" will consume memory in a single stream up to the number of blocks set by:  fs.s3a.multipart.size * fs.s3a.fast.upload.active.blocks. If using either of these mechanisms, keep this value low The total number of threads performing work across all threads is set by fs.s3a.threads.max, with fs.s3a.max.total.tasks values setting the number of queued work items.  ",The total number of threads available in the filesystem for data uploads *or any other queued filesystem operation*.
38,fs.s3a.fast.upload.buffer,fs.s3a.max.total.tasks," The buffering mechanism to use when using S3A fast upload (fs.s3a.fast.upload=true). Values: disk, array, bytebuffer. This configuration option has no effect if fs.s3a.fast.upload is false. ""disk"" will use the directories listed in fs.s3a.buffer.dir as the location(s) to save data prior to being uploaded. ""array"" uses arrays in the JVM heap ""bytebuffer"" uses off-heap memory within the JVM. Both ""array"" and ""bytebuffer"" will consume memory in a single stream up to the number of blocks set by:  fs.s3a.multipart.size * fs.s3a.fast.upload.active.blocks. If using either of these mechanisms, keep this value low The total number of threads performing work across all threads is set by fs.s3a.threads.max, with fs.s3a.max.total.tasks values setting the number of queued work items.  ",The number of operations which can be queued for execution
39,fs.s3a.fast.upload.buffer,fs.s3a.multipart.size," The buffering mechanism to use when using S3A fast upload (fs.s3a.fast.upload=true). Values: disk, array, bytebuffer. This configuration option has no effect if fs.s3a.fast.upload is false. ""disk"" will use the directories listed in fs.s3a.buffer.dir as the location(s) to save data prior to being uploaded. ""array"" uses arrays in the JVM heap ""bytebuffer"" uses off-heap memory within the JVM. Both ""array"" and ""bytebuffer"" will consume memory in a single stream up to the number of blocks set by:  fs.s3a.multipart.size * fs.s3a.fast.upload.active.blocks. If using either of these mechanisms, keep this value low The total number of threads performing work across all threads is set by fs.s3a.threads.max, with fs.s3a.max.total.tasks values setting the number of queued work items.  ","How big (in bytes) to split upload or copy operations up into. A suffix from the set {K,M,G,T,P} may be used to scale the numeric value.  "
40,fs.s3a.fast.upload.buffer,fs.s3a.buffer.dir," The buffering mechanism to use when using S3A fast upload (fs.s3a.fast.upload=true). Values: disk, array, bytebuffer. This configuration option has no effect if fs.s3a.fast.upload is false. ""disk"" will use the directories listed in fs.s3a.buffer.dir as the location(s) to save data prior to being uploaded. ""array"" uses arrays in the JVM heap ""bytebuffer"" uses off-heap memory within the JVM. Both ""array"" and ""bytebuffer"" will consume memory in a single stream up to the number of blocks set by:  fs.s3a.multipart.size * fs.s3a.fast.upload.active.blocks. If using either of these mechanisms, keep this value low The total number of threads performing work across all threads is set by fs.s3a.threads.max, with fs.s3a.max.total.tasks values setting the number of queued work items.  ",Comma separated list of directories that will be used to buffer file uploads to.
41,fs.s3a.fast.upload.buffer,fs.s3a.fast.upload," The buffering mechanism to use when using S3A fast upload (fs.s3a.fast.upload=true). Values: disk, array, bytebuffer. This configuration option has no effect if fs.s3a.fast.upload is false. ""disk"" will use the directories listed in fs.s3a.buffer.dir as the location(s) to save data prior to being uploaded. ""array"" uses arrays in the JVM heap ""bytebuffer"" uses off-heap memory within the JVM. Both ""array"" and ""bytebuffer"" will consume memory in a single stream up to the number of blocks set by:  fs.s3a.multipart.size * fs.s3a.fast.upload.active.blocks. If using either of these mechanisms, keep this value low The total number of threads performing work across all threads is set by fs.s3a.threads.max, with fs.s3a.max.total.tasks values setting the number of queued work items.  ", Use the incremental block-based fast upload mechanism with the buffering mechanism set in fs.s3a.fast.upload.buffer.  
42,fs.s3a.fast.upload.buffer,fs.s3a.fast.upload.active.blocks," The buffering mechanism to use when using S3A fast upload (fs.s3a.fast.upload=true). Values: disk, array, bytebuffer. This configuration option has no effect if fs.s3a.fast.upload is false. ""disk"" will use the directories listed in fs.s3a.buffer.dir as the location(s) to save data prior to being uploaded. ""array"" uses arrays in the JVM heap ""bytebuffer"" uses off-heap memory within the JVM. Both ""array"" and ""bytebuffer"" will consume memory in a single stream up to the number of blocks set by:  fs.s3a.multipart.size * fs.s3a.fast.upload.active.blocks. If using either of these mechanisms, keep this value low The total number of threads performing work across all threads is set by fs.s3a.threads.max, with fs.s3a.max.total.tasks values setting the number of queued work items.  "," Maximum Number of blocks a single output stream can have active (uploading, or queued to the central FileSystem instance's pool of queued operations. This stops a single stream overloading the shared thread pool.  "
43,fs.s3a.s3guard.ddb.table.capacity.write,fs.s3a.s3guard.ddb.table, Provisioned throughput requirements for write operations in terms of capacity units for the DynamoDB table.  Refer to related config fs.s3a.s3guard.ddb.table.capacity.read before usage.  ," The DynamoDB table name to operate. Without this property, the respective S3 bucket name will be used.  "
44,fs.s3a.s3guard.ddb.table.capacity.write,fs.s3a.s3guard.ddb.table.capacity.read, Provisioned throughput requirements for write operations in terms of capacity units for the DynamoDB table.  Refer to related config fs.s3a.s3guard.ddb.table.capacity.read before usage.  ," Provisioned throughput requirements for read operations in terms of capacity units for the DynamoDB table.  This config value will only be used when creating a new DynamoDB table, though later you can manually provision by increasing or decreasing read capacity as needed for existing tables. See DynamoDB documents for more information.  "
45,fs.azure.local.sas.key.mode,fs.azure.secure.mode," Works in conjuction with fs.azure.secure.mode. Setting this config to true results in fs.azure.NativeAzureFileSystem using the local SAS key generation where the SAS keys are generating in the same process as fs.azure.NativeAzureFileSystem. If fs.azure.secure.mode flag is set to false, this flag has no effect.  "," Config flag to identify the mode in which fs.azure.NativeAzureFileSystem needs to run under. Setting it ""true"" would make fs.azure.NativeAzureFileSystem use SAS keys to communicate with Azure storage.  "
46,fs.azure.authorization.caching.enable,fs.azure.authorization, Config flag to enable caching of authorization results and saskeys in WASB. This flag is relevant only when fs.azure.authorization is enabled.  ," Config flag to enable authorization support in WASB. Setting it to ""true"" enables authorization support to WASB. Currently WASB authorization requires a remote service to provide authorization that needs to be specified via fs.azure.authorization.remote.service.url configuration  "
47,ipc.client.ping,ipc.client.rpc-timeout.ms,"Send a ping to the server when timeout on reading the response,  if set to true. If no failure is detected, the client retries until at least  a byte is read or the time given by ipc.client.rpc-timeout.ms is passed.  ","Timeout on waiting response from server, in milliseconds.  If ipc.client.ping is set to true and this rpc-timeout is greater than  the value of ipc.ping.interval, the effective value of the rpc-timeout is  rounded up to multiple of ipc.ping.interval.  "
48,ipc.ping.interval,ipc.client.ping,"Timeout on waiting response from server, in milliseconds.  The client will send ping when the interval is passed without receiving bytes,  if ipc.client.ping is set to true.  ","Send a ping to the server when timeout on reading the response,  if set to true. If no failure is detected, the client retries until at least  a byte is read or the time given by ipc.client.rpc-timeout.ms is passed.  "
49,ipc.client.rpc-timeout.ms,ipc.client.ping,"Timeout on waiting response from server, in milliseconds.  If ipc.client.ping is set to true and this rpc-timeout is greater than  the value of ipc.ping.interval, the effective value of the rpc-timeout is  rounded up to multiple of ipc.ping.interval.  ","Send a ping to the server when timeout on reading the response,  if set to true. If no failure is detected, the client retries until at least  a byte is read or the time given by ipc.client.rpc-timeout.ms is passed.  "
50,ipc.client.rpc-timeout.ms,ipc.ping.interval,"Timeout on waiting response from server, in milliseconds.  If ipc.client.ping is set to true and this rpc-timeout is greater than  the value of ipc.ping.interval, the effective value of the rpc-timeout is  rounded up to multiple of ipc.ping.interval.  ","Timeout on waiting response from server, in milliseconds.  The client will send ping when the interval is passed without receiving bytes,  if ipc.client.ping is set to true.  "
51,net.topology.node.switch.mapping.impl,net.topology.script.file.name," The default implementation of the DNSToSwitchMapping. It invokes a script specified in net.topology.script.file.name to resolve node names. If the value for net.topology.script.file.name is not set, the default value of DEFAULT_RACK is returned for all node names.  "," The script name that should be invoked to resolve DNS names to NetworkTopology names. Example: the script would take host.foo.bar as an argument, and return /rack1 as the output.  "
52,net.topology.script.number.args,net.topology.script.file.name, The max number of args that the script configured with net.topology.script.file.name should be run with. Each arg is an IP address.  ," The script name that should be invoked to resolve DNS names to NetworkTopology names. Example: the script would take host.foo.bar as an argument, and return /rack1 as the output.  "
53,net.topology.table.file.name,net.topology.node.switch.mapping.impl," The file name for a topology file, which is used when the net.topology.node.switch.mapping.impl property is set to org.apache.hadoop.net.TableMapping. The file format is a two column text file, with columns separated by whitespace. The first column is a DNS or IP address and the second column specifies the rack where the address maps. If no entry corresponding to a host in the cluster is found, then /default-rack is assumed.  "," The default implementation of the DNSToSwitchMapping. It invokes a script specified in net.topology.script.file.name to resolve node names. If the value for net.topology.script.file.name is not set, the default value of DEFAULT_RACK is returned for all node names.  "
54,file.bytes-per-checksum,file.stream-buffer-size,The number of bytes per checksum.  Must not be larger than  file.stream-buffer-size,"The size of buffer to stream files.  The size of this buffer should probably be a multiple of hardware  page size (4096 on Intel x86), and it determines how much data is  buffered during read and write operations."
55,s3.bytes-per-checksum,s3.stream-buffer-size,The number of bytes per checksum.  Must not be larger than  s3.stream-buffer-size,"The size of buffer to stream files.  The size of this buffer should probably be a multiple of hardware  page size (4096 on Intel x86), and it determines how much data is  buffered during read and write operations."
56,s3native.bytes-per-checksum,s3native.stream-buffer-size,The number of bytes per checksum.  Must not be larger than  s3native.stream-buffer-size,"The size of buffer to stream files.  The size of this buffer should probably be a multiple of hardware  page size (4096 on Intel x86), and it determines how much data is  buffered during read and write operations."
57,ftp.bytes-per-checksum,ftp.stream-buffer-size,The number of bytes per checksum.  Must not be larger than  ftp.stream-buffer-size,"The size of buffer to stream files.  The size of this buffer should probably be a multiple of hardware  page size (4096 on Intel x86), and it determines how much data is  buffered during read and write operations."
58,ha.zookeeper.auth,ha.zookeeper.acl," A comma-separated list of ZooKeeper authentications to add when connecting to ZooKeeper. These are specified in the same format as used by the ""addauth"" command in the ZK CLI. It is important that the authentications specified here are sufficient to access znodes with the ACL specified in ha.zookeeper.acl. If the auths contain secrets, you may instead specify a path to a file, prefixed with the '@' symbol, and the value of this configuration will be loaded from within.  "," A comma-separated list of ZooKeeper ACLs to apply to the znodes used by automatic failover. These ACLs are specified in the same format as used by the ZooKeeper CLI. If the ACL itself contains secrets, you may instead specify a path to a file, prefixed with the '@' symbol, and the value of this configuration will be loaded from within.  "
59,rpc.metrics.quantile.enable,rpc.metrics.percentiles.intervals," Setting this property to true and rpc.metrics.percentiles.intervals to a comma-separated list of the granularity in seconds, the 50/75/90/95/99th percentile latency for rpc queue/processing time in milliseconds are added to rpc metrics.  ", A comma-separated list of the granularity in seconds for the metrics which describe the 50/75/90/95/99th percentile latency for rpc queue/processing time. The metrics are outputted if rpc.metrics.quantile.enable is set to true.  
60,rpc.metrics.percentiles.intervals,rpc.metrics.quantile.enable, A comma-separated list of the granularity in seconds for the metrics which describe the 50/75/90/95/99th percentile latency for rpc queue/processing time. The metrics are outputted if rpc.metrics.quantile.enable is set to true.  ," Setting this property to true and rpc.metrics.percentiles.intervals to a comma-separated list of the granularity in seconds, the 50/75/90/95/99th percentile latency for rpc queue/processing time in milliseconds are added to rpc metrics.  "
61,hadoop.shell.missing.defaultFs.warning,fs.defaultFS,   Enable hdfs shell commands to display warnings if (fs.defaultFS) property   is not set. ,"The name of the default file system.  A URI whose  scheme and authority determine the FileSystem implementation.  The  uri's scheme determines the config property (fs.SCHEME.impl) naming  the FileSystem implementation class.  The uri's authority is used to  determine the host, port, etc. for a filesystem."
62,fs.client.resolve.topology.enabled,net.topology.node.switch.mapping.impl,"Whether the client machine will use the class specified by   property net.topology.node.switch.mapping.impl to compute the network   distance between itself and remote machines of the FileSystem. Additional   properties might need to be configured depending on the class specified   in net.topology.node.switch.mapping.impl. For example, if   org.apache.hadoop.net.ScriptBasedMapping is used, a valid script file   needs to be specified in net.topology.script.file.name. "," The default implementation of the DNSToSwitchMapping. It invokes a script specified in net.topology.script.file.name to resolve node names. If the value for net.topology.script.file.name is not set, the default value of DEFAULT_RACK is returned for all node names.  "
63,fs.client.resolve.topology.enabled,net.topology.script.file.name,"Whether the client machine will use the class specified by   property net.topology.node.switch.mapping.impl to compute the network   distance between itself and remote machines of the FileSystem. Additional   properties might need to be configured depending on the class specified   in net.topology.node.switch.mapping.impl. For example, if   org.apache.hadoop.net.ScriptBasedMapping is used, a valid script file   needs to be specified in net.topology.script.file.name. "," The script name that should be invoked to resolve DNS names to NetworkTopology names. Example: the script would take host.foo.bar as an argument, and return /rack1 as the output.  "
64,fs.adl.oauth2.access.token.provider.type,fs.adl.oauth2.client.id,"   Defines Azure Active Directory OAuth2 access token provider type.   Supported types are ClientCredential, RefreshToken, MSI, DeviceCode,   and Custom.   The ClientCredential type requires property fs.adl.oauth2.client.id,   fs.adl.oauth2.credential, and fs.adl.oauth2.refresh.url.   The RefreshToken type requires property fs.adl.oauth2.client.id and   fs.adl.oauth2.refresh.token.   The MSI type reads optional property fs.adl.oauth2.msi.port, if specified.   The DeviceCode type requires property   fs.adl.oauth2.devicecode.clientapp.id.   The Custom type requires property fs.adl.oauth2.access.token.provider. ",The OAuth2 client id.
65,fs.adl.oauth2.access.token.provider.type,fs.adl.oauth2.credential,"   Defines Azure Active Directory OAuth2 access token provider type.   Supported types are ClientCredential, RefreshToken, MSI, DeviceCode,   and Custom.   The ClientCredential type requires property fs.adl.oauth2.client.id,   fs.adl.oauth2.credential, and fs.adl.oauth2.refresh.url.   The RefreshToken type requires property fs.adl.oauth2.client.id and   fs.adl.oauth2.refresh.token.   The MSI type reads optional property fs.adl.oauth2.msi.port, if specified.   The DeviceCode type requires property   fs.adl.oauth2.devicecode.clientapp.id.   The Custom type requires property fs.adl.oauth2.access.token.provider. ",The OAuth2 access key.
66,fs.adl.oauth2.access.token.provider.type,fs.adl.oauth2.refresh.url,"   Defines Azure Active Directory OAuth2 access token provider type.   Supported types are ClientCredential, RefreshToken, MSI, DeviceCode,   and Custom.   The ClientCredential type requires property fs.adl.oauth2.client.id,   fs.adl.oauth2.credential, and fs.adl.oauth2.refresh.url.   The RefreshToken type requires property fs.adl.oauth2.client.id and   fs.adl.oauth2.refresh.token.   The MSI type reads optional property fs.adl.oauth2.msi.port, if specified.   The DeviceCode type requires property   fs.adl.oauth2.devicecode.clientapp.id.   The Custom type requires property fs.adl.oauth2.access.token.provider. ",The OAuth2 token endpoint.
67,fs.adl.oauth2.access.token.provider.type,fs.adl.oauth2.refresh.token,"   Defines Azure Active Directory OAuth2 access token provider type.   Supported types are ClientCredential, RefreshToken, MSI, DeviceCode,   and Custom.   The ClientCredential type requires property fs.adl.oauth2.client.id,   fs.adl.oauth2.credential, and fs.adl.oauth2.refresh.url.   The RefreshToken type requires property fs.adl.oauth2.client.id and   fs.adl.oauth2.refresh.token.   The MSI type reads optional property fs.adl.oauth2.msi.port, if specified.   The DeviceCode type requires property   fs.adl.oauth2.devicecode.clientapp.id.   The Custom type requires property fs.adl.oauth2.access.token.provider. ",The OAuth2 refresh token.
68,fs.adl.oauth2.access.token.provider.type,fs.adl.oauth2.access.token.provider,"   Defines Azure Active Directory OAuth2 access token provider type.   Supported types are ClientCredential, RefreshToken, MSI, DeviceCode,   and Custom.   The ClientCredential type requires property fs.adl.oauth2.client.id,   fs.adl.oauth2.credential, and fs.adl.oauth2.refresh.url.   The RefreshToken type requires property fs.adl.oauth2.client.id and   fs.adl.oauth2.refresh.token.   The MSI type reads optional property fs.adl.oauth2.msi.port, if specified.   The DeviceCode type requires property   fs.adl.oauth2.devicecode.clientapp.id.   The Custom type requires property fs.adl.oauth2.access.token.provider. ",   The class name of the OAuth2 access token provider. 
69,fs.adl.oauth2.access.token.provider.type,fs.adl.oauth2.msi.port,"   Defines Azure Active Directory OAuth2 access token provider type.   Supported types are ClientCredential, RefreshToken, MSI, DeviceCode,   and Custom.   The ClientCredential type requires property fs.adl.oauth2.client.id,   fs.adl.oauth2.credential, and fs.adl.oauth2.refresh.url.   The RefreshToken type requires property fs.adl.oauth2.client.id and   fs.adl.oauth2.refresh.token.   The MSI type reads optional property fs.adl.oauth2.msi.port, if specified.   The DeviceCode type requires property   fs.adl.oauth2.devicecode.clientapp.id.   The Custom type requires property fs.adl.oauth2.access.token.provider. ","   The localhost port for the MSI token service. This is the port specified   when creating the Azure VM. The default, if this setting is not specified,   is 50342.   Used by MSI token provider. "
70,fs.adl.oauth2.access.token.provider.type,fs.adl.oauth2.devicecode.clientapp.id,"   Defines Azure Active Directory OAuth2 access token provider type.   Supported types are ClientCredential, RefreshToken, MSI, DeviceCode,   and Custom.   The ClientCredential type requires property fs.adl.oauth2.client.id,   fs.adl.oauth2.credential, and fs.adl.oauth2.refresh.url.   The RefreshToken type requires property fs.adl.oauth2.client.id and   fs.adl.oauth2.refresh.token.   The MSI type reads optional property fs.adl.oauth2.msi.port, if specified.   The DeviceCode type requires property   fs.adl.oauth2.devicecode.clientapp.id.   The Custom type requires property fs.adl.oauth2.access.token.provider. ",   The app id of the AAD native app in whose context the auth request   should be made.   Used by DeviceCode token provider. 
71,hadoop.zk.auth,hadoop.zk.acl,"  Specify the auths to be used for the ACL's specified in hadoop.zk.acl.  This takes a comma-separated list of authentication mechanisms, each of the  form 'scheme:auth' (the same syntax used for the 'addAuth' command in  the ZK CLI). ",ACL's to be used for ZooKeeper znodes.
