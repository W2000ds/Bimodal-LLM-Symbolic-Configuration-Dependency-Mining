ID,Option A,Option B,Option A Description,Option B Description
1,mapreduce.job.local-fs.single-disk-limit.check.kill-limit-exceed,mapreduce.job.local-fs.single-disk-limit.bytes,If mapreduce.job.local-fs.single-disk-limit.bytes is triggered should the task be killed or logged. If false the intent to kill the task is only logged in the container logs.,"Enable an in task monitor thread to watch for single disk consumption by jobs. By setting this to x nr of bytes, the task will fast fail in case it is reached. This is a per disk configuration."
2,mapreduce.job.maps,mapreduce.framework.name,"The default number of map tasks per job. Ignored when mapreduce.framework.name is ""local"".","The runtime framework for executing MapReduce jobs. Can be one of local, classic or yarn."
3,mapreduce.job.reduces,mapreduce.framework.name,"The default number of reduce tasks per job. Typically set to 99% of the cluster's reduce capacity, so that if a node fails the reduces can still be executed in a single wave. Ignored when mapreduce.framework.name is ""local"".","The runtime framework for executing MapReduce jobs. Can be one of local, classic or yarn."
4,mapreduce.map.memory.mb,mapreduce.job.heap.memory-mb.ratio,"The amount of memory to request from the scheduler for each map task. If this is not specified or is non-positive, it is inferred from mapreduce.map.java.opts and mapreduce.job.heap.memory-mb.ratio. If java-opts are also not specified, we set it to 1024.","The ratio of heap-size to container-size. If no -Xmx is specified, it is calculated as (mapreduce.{map|reduce}.memory.mb * mapreduce.heap.memory-mb.ratio). If -Xmx is specified but not mapreduce.{map|reduce}.memory.mb, it is calculated as (heapSize / mapreduce.heap.memory-mb.ratio)."
5,mapreduce.reduce.memory.mb,mapreduce.job.heap.memory-mb.ratio,"The amount of memory to request from the scheduler for each reduce task. If this is not specified or is non-positive, it is inferred from mapreduce.reduce.java.opts and mapreduce.job.heap.memory-mb.ratio. If java-opts are also not specified, we set it to 1024.","The ratio of heap-size to container-size. If no -Xmx is specified, it is calculated as (mapreduce.{map|reduce}.memory.mb * mapreduce.heap.memory-mb.ratio). If -Xmx is specified but not mapreduce.{map|reduce}.memory.mb, it is calculated as (heapSize / mapreduce.heap.memory-mb.ratio)."
6,mapred.child.java.opts,mapreduce.job.heap.memory-mb.ratio,"Java opts for the task processes. The following symbol, if present, will be interpolated: @taskid@ is replaced by current TaskID. Any other occurrences of '@' will go unchanged. For example, to enable verbose gc logging to a file named for the taskid in /tmp and to set the heap maximum to be a gigabyte, pass a 'value' of: -Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc Usage of -Djava.library.path can cause programs to no longer function if hadoop native libraries are used. These values should instead be set as part of LD_LIBRARY_PATH in the map / reduce JVM env using the mapreduce.map.env and mapreduce.reduce.env config settings. If -Xmx is not set, it is inferred from mapreduce.{map|reduce}.memory.mb and mapreduce.job.heap.memory-mb.ratio.","The ratio of heap-size to container-size. If no -Xmx is specified, it is calculated as (mapreduce.{map|reduce}.memory.mb * mapreduce.heap.memory-mb.ratio). If -Xmx is specified but not mapreduce.{map|reduce}.memory.mb, it is calculated as (heapSize / mapreduce.heap.memory-mb.ratio)."
7,yarn.app.mapreduce.am.log.level,mapreduce.job.log4j-properties-file,"The logging level for the MR ApplicationMaster. The allowed levels are: OFF, FATAL, ERROR, WARN, INFO, DEBUG, TRACE and ALL. The setting here could be overriden if ""mapreduce.job.log4j-properties-file"" is set.","Used to override the default settings of log4j in container-log4j.properties for NodeManager. Like container-log4j.properties, it requires certain framework appenders properly defined in this overriden file. The file on the path will be added to distributed cache and classpath. If no-scheme is given in the path, it defaults to point to a log4j file on the local FS."
8,mapreduce.map.log.level,mapreduce.job.log4j-properties-file,"The logging level for the map task. The allowed levels are: OFF, FATAL, ERROR, WARN, INFO, DEBUG, TRACE and ALL. The setting here could be overridden if ""mapreduce.job.log4j-properties-file"" is set.","Used to override the default settings of log4j in container-log4j.properties for NodeManager. Like container-log4j.properties, it requires certain framework appenders properly defined in this overriden file. The file on the path will be added to distributed cache and classpath. If no-scheme is given in the path, it defaults to point to a log4j file on the local FS."
9,mapreduce.reduce.log.level,mapreduce.job.log4j-properties-file,"The logging level for the reduce task. The allowed levels are: OFF, FATAL, ERROR, WARN, INFO, DEBUG, TRACE and ALL. The setting here could be overridden if ""mapreduce.job.log4j-properties-file"" is set.","Used to override the default settings of log4j in container-log4j.properties for NodeManager. Like container-log4j.properties, it requires certain framework appenders properly defined in this overriden file. The file on the path will be added to distributed cache and classpath. If no-scheme is given in the path, it defaults to point to a log4j file on the local FS."
10,mapreduce.reduce.shuffle.merge.percent,mapreduce.reduce.shuffle.input.buffer.percent,"The usage threshold at which an in-memory merge will be initiated, expressed as a percentage of the total memory allocated to storing in-memory map outputs, as defined by mapreduce.reduce.shuffle.input.buffer.percent.",The percentage of memory to be allocated from the maximum heap size to storing map outputs during the shuffle.
11,mapreduce.shuffle.transfer.buffer.size,mapreduce.shuffle.transferTo.allowed,"This property is used only if mapreduce.shuffle.transferTo.allowed is set to false. In that case, this property defines the size of the buffer used in the buffer copy code for the shuffle phase. The size of this buffer determines the size of the IO requests.","This option can enable/disable using nio transferTo method in the shuffle phase. NIO transferTo does not perform well on windows in the shuffle phase. Thus, with this configuration property it is possible to disable it, in which case custom transfer method will be used. Recommended value is false when running Hadoop on Windows. For Linux, it is recommended to set it to true. If nothing is set then the default value is false for Windows, and true for Linux."
12,mapreduce.job.ubertask.enable,mapreduce.map.memory.mb,"Whether to enable the small-jobs ""ubertask"" optimization, which runs ""sufficiently small"" jobs sequentially within a single JVM. ""Small"" is defined by the following maxmaps, maxreduces, and maxbytes settings. Note that configurations for application masters also affect the ""Small"" definition - yarn.app.mapreduce.am.resource.mb must be larger than both mapreduce.map.memory.mb and mapreduce.reduce.memory.mb, and yarn.app.mapreduce.am.resource.cpu-vcores must be larger than both mapreduce.map.cpu.vcores and mapreduce.reduce.cpu.vcores to enable ubertask. Users may override this value.","The amount of memory to request from the scheduler for each map task. If this is not specified or is non-positive, it is inferred from mapreduce.map.java.opts and mapreduce.job.heap.memory-mb.ratio. If java-opts are also not specified, we set it to 1024."
13,mapreduce.job.ubertask.enable,mapreduce.map.cpu.vcores,"Whether to enable the small-jobs ""ubertask"" optimization, which runs ""sufficiently small"" jobs sequentially within a single JVM. ""Small"" is defined by the following maxmaps, maxreduces, and maxbytes settings. Note that configurations for application masters also affect the ""Small"" definition - yarn.app.mapreduce.am.resource.mb must be larger than both mapreduce.map.memory.mb and mapreduce.reduce.memory.mb, and yarn.app.mapreduce.am.resource.cpu-vcores must be larger than both mapreduce.map.cpu.vcores and mapreduce.reduce.cpu.vcores to enable ubertask. Users may override this value.",The number of virtual cores to request from the scheduler for each map task.
14,mapreduce.job.ubertask.enable,mapreduce.reduce.memory.mb,"Whether to enable the small-jobs ""ubertask"" optimization, which runs ""sufficiently small"" jobs sequentially within a single JVM. ""Small"" is defined by the following maxmaps, maxreduces, and maxbytes settings. Note that configurations for application masters also affect the ""Small"" definition - yarn.app.mapreduce.am.resource.mb must be larger than both mapreduce.map.memory.mb and mapreduce.reduce.memory.mb, and yarn.app.mapreduce.am.resource.cpu-vcores must be larger than both mapreduce.map.cpu.vcores and mapreduce.reduce.cpu.vcores to enable ubertask. Users may override this value.","The amount of memory to request from the scheduler for each reduce task. If this is not specified or is non-positive, it is inferred from mapreduce.reduce.java.opts and mapreduce.job.heap.memory-mb.ratio. If java-opts are also not specified, we set it to 1024."
15,mapreduce.job.ubertask.enable,mapreduce.reduce.cpu.vcores,"Whether to enable the small-jobs ""ubertask"" optimization, which runs ""sufficiently small"" jobs sequentially within a single JVM. ""Small"" is defined by the following maxmaps, maxreduces, and maxbytes settings. Note that configurations for application masters also affect the ""Small"" definition - yarn.app.mapreduce.am.resource.mb must be larger than both mapreduce.map.memory.mb and mapreduce.reduce.memory.mb, and yarn.app.mapreduce.am.resource.cpu-vcores must be larger than both mapreduce.map.cpu.vcores and mapreduce.reduce.cpu.vcores to enable ubertask. Users may override this value.",The number of virtual cores to request from the scheduler for each reduce task.
16,mapreduce.job.ubertask.enable,yarn.app.mapreduce.am.resource.mb,"Whether to enable the small-jobs ""ubertask"" optimization, which runs ""sufficiently small"" jobs sequentially within a single JVM. ""Small"" is defined by the following maxmaps, maxreduces, and maxbytes settings. Note that configurations for application masters also affect the ""Small"" definition - yarn.app.mapreduce.am.resource.mb must be larger than both mapreduce.map.memory.mb and mapreduce.reduce.memory.mb, and yarn.app.mapreduce.am.resource.cpu-vcores must be larger than both mapreduce.map.cpu.vcores and mapreduce.reduce.cpu.vcores to enable ubertask. Users may override this value.",The amount of memory the MR AppMaster needs.
17,mapreduce.job.ubertask.enable,yarn.app.mapreduce.am.resource.cpu-vcores,"Whether to enable the small-jobs ""ubertask"" optimization, which runs ""sufficiently small"" jobs sequentially within a single JVM. ""Small"" is defined by the following maxmaps, maxreduces, and maxbytes settings. Note that configurations for application masters also affect the ""Small"" definition - yarn.app.mapreduce.am.resource.mb must be larger than both mapreduce.map.memory.mb and mapreduce.reduce.memory.mb, and yarn.app.mapreduce.am.resource.cpu-vcores must be larger than both mapreduce.map.cpu.vcores and mapreduce.reduce.cpu.vcores to enable ubertask. Users may override this value.",The number of virtual CPU cores the MR AppMaster needs.
18,yarn.app.mapreduce.task.container.log.backups,mapreduce.task.userlog.limit.kb,"Number of backup files for task logs when using ContainerRollingLogAppender (CRLA). See org.apache.log4j.RollingFileAppender.maxBackupIndex. By default, ContainerLogAppender (CLA) is used, and container logs are not rolled. CRLA is enabled for tasks when both mapreduce.task.userlog.limit.kb and yarn.app.mapreduce.task.container.log.backups are greater than zero.",The maximum size of user-logs of each task in KB. 0 disables the cap.
19,yarn.app.mapreduce.am.container.log.backups,yarn.app.mapreduce.am.container.log.limit.kb,"Number of backup files for the ApplicationMaster logs when using ContainerRollingLogAppender (CRLA). See org.apache.log4j.RollingFileAppender.maxBackupIndex. By default, ContainerLogAppender (CLA) is used, and container logs are not rolled. CRLA is enabled for the ApplicationMaster when both yarn.app.mapreduce.am.container.log.limit.kb and yarn.app.mapreduce.am.container.log.backups are greater than zero.",The maximum size of the MRAppMaster attempt container logs in KB. 0 disables the cap.
20,yarn.app.mapreduce.shuffle.log.backups,yarn.app.mapreduce.shuffle.log.limit.kb,If yarn.app.mapreduce.shuffle.log.limit.kb and yarn.app.mapreduce.shuffle.log.backups are greater than zero then a ContainerRollngLogAppender is used instead of ContainerLogAppender for syslog.shuffle. See org.apache.log4j.RollingFileAppender.maxBackupIndex,Maximum size of the syslog.shuffle file in kilobytes (0 for no limit).
21,mapreduce.job.maxtaskfailures.per.tracker,mapreduce.map.maxattempts,The number of task-failures on a node manager of a given job after which new tasks of that job aren't assigned to it. It MUST be less than mapreduce.map.maxattempts and mapreduce.reduce.maxattempts otherwise the failed task will never be tried on a different node.,"Expert: The maximum number of attempts per map task. In other words, framework will try to execute a map task these many number of times before giving up on it."
22,mapreduce.job.maxtaskfailures.per.tracker,mapreduce.reduce.maxattempts,The number of task-failures on a node manager of a given job after which new tasks of that job aren't assigned to it. It MUST be less than mapreduce.map.maxattempts and mapreduce.reduce.maxattempts otherwise the failed task will never be tried on a different node.,"Expert: The maximum number of attempts per reduce task. In other words, framework will try to execute a reduce task these many number of times before giving up on it."
23,mapreduce.task.profile.maps,mapreduce.task.profile,To set the ranges of map tasks to profile. mapreduce.task.profile has to be set to true for the value to be accounted.,"To set whether the system should collect profiler information for some of the tasks in this job? The information is stored in the user log directory. The value is ""true"" if task profiling is enabled."
24,mapreduce.task.profile.reduces,mapreduce.task.profile,To set the ranges of reduce tasks to profile. mapreduce.task.profile has to be set to true for the value to be accounted.,"To set whether the system should collect profiler information for some of the tasks in this job? The information is stored in the user log directory. The value is ""true"" if task profiling is enabled."
25,mapreduce.task.profile.params,mapreduce.task.profile,"JVM profiler parameters used to profile map and reduce task attempts. This string may contain a single format specifier %s that will be replaced by the path to profile.out in the task attempt log directory. To specify different profiling options for map tasks and reduce tasks, more specific parameters mapreduce.task.profile.map.params and mapreduce.task.profile.reduce.params should be used.","To set whether the system should collect profiler information for some of the tasks in this job? The information is stored in the user log directory. The value is ""true"" if task profiling is enabled."
26,mapreduce.task.profile.params,mapreduce.task.profile.map.params,"JVM profiler parameters used to profile map and reduce task attempts. This string may contain a single format specifier %s that will be replaced by the path to profile.out in the task attempt log directory. To specify different profiling options for map tasks and reduce tasks, more specific parameters mapreduce.task.profile.map.params and mapreduce.task.profile.reduce.params should be used.",Map-task-specific JVM profiler parameters. See mapreduce.task.profile.params
27,mapreduce.task.profile.params,mapreduce.task.profile.reduce.params,"JVM profiler parameters used to profile map and reduce task attempts. This string may contain a single format specifier %s that will be replaced by the path to profile.out in the task attempt log directory. To specify different profiling options for map tasks and reduce tasks, more specific parameters mapreduce.task.profile.map.params and mapreduce.task.profile.reduce.params should be used.",Reduce-task-specific JVM profiler parameters. See mapreduce.task.profile.params
28,mapreduce.task.profile.map.params,mapreduce.task.profile,Map-task-specific JVM profiler parameters. See mapreduce.task.profile.params,"To set whether the system should collect profiler information for some of the tasks in this job? The information is stored in the user log directory. The value is ""true"" if task profiling is enabled."
29,mapreduce.task.profile.map.params,mapreduce.task.profile.params,Map-task-specific JVM profiler parameters. See mapreduce.task.profile.params,"JVM profiler parameters used to profile map and reduce task attempts. This string may contain a single format specifier %s that will be replaced by the path to profile.out in the task attempt log directory. To specify different profiling options for map tasks and reduce tasks, more specific parameters mapreduce.task.profile.map.params and mapreduce.task.profile.reduce.params should be used."
30,mapreduce.task.profile.reduce.params,mapreduce.task.profile,Reduce-task-specific JVM profiler parameters. See mapreduce.task.profile.params,"To set whether the system should collect profiler information for some of the tasks in this job? The information is stored in the user log directory. The value is ""true"" if task profiling is enabled."
31,mapreduce.task.profile.reduce.params,mapreduce.task.profile.params,Reduce-task-specific JVM profiler parameters. See mapreduce.task.profile.params,"JVM profiler parameters used to profile map and reduce task attempts. This string may contain a single format specifier %s that will be replaced by the path to profile.out in the task attempt log directory. To specify different profiling options for map tasks and reduce tasks, more specific parameters mapreduce.task.profile.map.params and mapreduce.task.profile.reduce.params should be used."
32,mapreduce.cluster.acls.enabled,mapreduce.job.acl-modify-job,"Specifies whether ACLs should be checked for authorization of users for doing various queue and job level operations. ACLs are disabled by default. If enabled, access control checks are made by MapReduce ApplicationMaster when requests are made by users for queue operations like submit job to a queue and kill a job in the queue and job operations like viewing the job-details (See mapreduce.job.acl-view-job) or for modifying the job (See mapreduce.job.acl-modify-job) using Map/Reduce APIs, RPCs or via the console and web user interfaces. For enabling this flag, set to true in mapred-site.xml file of all MapReduce clients (MR job submitting nodes).","Job specific access-control list for 'modifying' the job. It is only used if authorization is enabled in Map/Reduce by setting the configuration property mapreduce.cluster.acls.enabled to true. This specifies the list of users and/or groups who can do modification operations on the job. For specifying a list of users and groups the format to use is ""user1,user2 group1,group"". If set to '*', it allows all users/groups to modify this job. If set to ' '(i.e. space), it allows none. This configuration is used to guard all the modifications with respect to this job and takes care of all the following operations: o killing this job o killing a task of this job, failing a task of this job o setting the priority of this job Each of these operations are also protected by the per-queue level ACL ""acl-administer-jobs"" configured via mapred-queues.xml. So a caller should have the authorization to satisfy either the queue-level ACL or the job-level ACL. Irrespective of this ACL configuration, (a) job-owner, (b) the user who started the cluster, (c) members of an admin configured supergroup configured via mapreduce.cluster.permissions.supergroup and (d) queue administrators of the queue to which this job was submitted to configured via acl-administer-jobs for the specific queue in mapred-queues.xml can do all the modification operations on a job. By default, nobody else besides job-owner, the user who started the cluster, members of supergroup and queue administrators can perform modification operations on a job."
33,mapreduce.cluster.acls.enabled,mapreduce.job.acl-view-job,"Specifies whether ACLs should be checked for authorization of users for doing various queue and job level operations. ACLs are disabled by default. If enabled, access control checks are made by MapReduce ApplicationMaster when requests are made by users for queue operations like submit job to a queue and kill a job in the queue and job operations like viewing the job-details (See mapreduce.job.acl-view-job) or for modifying the job (See mapreduce.job.acl-modify-job) using Map/Reduce APIs, RPCs or via the console and web user interfaces. For enabling this flag, set to true in mapred-site.xml file of all MapReduce clients (MR job submitting nodes).","Job specific access-control list for 'viewing' the job. It is only used if authorization is enabled in Map/Reduce by setting the configuration property mapreduce.cluster.acls.enabled to true. This specifies the list of users and/or groups who can view private details about the job. For specifying a list of users and groups the format to use is ""user1,user2 group1,group"". If set to '*', it allows all users/groups to modify this job. If set to ' '(i.e. space), it allows none. This configuration is used to guard some of the job-views and at present only protects APIs that can return possibly sensitive information of the job-owner like o job-level counters o task-level counters o tasks' diagnostic information o task-logs displayed on the HistoryServer's web-UI and o job.xml showed by the HistoryServer's web-UI Every other piece of information of jobs is still accessible by any other user, for e.g., JobStatus, JobProfile, list of jobs in the queue, etc. Irrespective of this ACL configuration, (a) job-owner, (b) the user who started the cluster, (c) members of an admin configured supergroup configured via mapreduce.cluster.permissions.supergroup and (d) queue administrators of the queue to which this job was submitted to configured via acl-administer-jobs for the specific queue in mapred-queues.xml can do all the view operations on a job. By default, nobody else besides job-owner, the user who started the cluster, memebers of supergroup and queue administrators can perform view operations on a job."
34,mapreduce.job.acl-modify-job,mapreduce.cluster.acls.enabled,"Job specific access-control list for 'modifying' the job. It is only used if authorization is enabled in Map/Reduce by setting the configuration property mapreduce.cluster.acls.enabled to true. This specifies the list of users and/or groups who can do modification operations on the job. For specifying a list of users and groups the format to use is ""user1,user2 group1,group"". If set to '*', it allows all users/groups to modify this job. If set to ' '(i.e. space), it allows none. This configuration is used to guard all the modifications with respect to this job and takes care of all the following operations: o killing this job o killing a task of this job, failing a task of this job o setting the priority of this job Each of these operations are also protected by the per-queue level ACL ""acl-administer-jobs"" configured via mapred-queues.xml. So a caller should have the authorization to satisfy either the queue-level ACL or the job-level ACL. Irrespective of this ACL configuration, (a) job-owner, (b) the user who started the cluster, (c) members of an admin configured supergroup configured via mapreduce.cluster.permissions.supergroup and (d) queue administrators of the queue to which this job was submitted to configured via acl-administer-jobs for the specific queue in mapred-queues.xml can do all the modification operations on a job. By default, nobody else besides job-owner, the user who started the cluster, members of supergroup and queue administrators can perform modification operations on a job.","Specifies whether ACLs should be checked for authorization of users for doing various queue and job level operations. ACLs are disabled by default. If enabled, access control checks are made by MapReduce ApplicationMaster when requests are made by users for queue operations like submit job to a queue and kill a job in the queue and job operations like viewing the job-details (See mapreduce.job.acl-view-job) or for modifying the job (See mapreduce.job.acl-modify-job) using Map/Reduce APIs, RPCs or via the console and web user interfaces. For enabling this flag, set to true in mapred-site.xml file of all MapReduce clients (MR job submitting nodes)."
35,mapreduce.job.acl-view-job,mapreduce.cluster.acls.enabled,"Job specific access-control list for 'viewing' the job. It is only used if authorization is enabled in Map/Reduce by setting the configuration property mapreduce.cluster.acls.enabled to true. This specifies the list of users and/or groups who can view private details about the job. For specifying a list of users and groups the format to use is ""user1,user2 group1,group"". If set to '*', it allows all users/groups to modify this job. If set to ' '(i.e. space), it allows none. This configuration is used to guard some of the job-views and at present only protects APIs that can return possibly sensitive information of the job-owner like o job-level counters o task-level counters o tasks' diagnostic information o task-logs displayed on the HistoryServer's web-UI and o job.xml showed by the HistoryServer's web-UI Every other piece of information of jobs is still accessible by any other user, for e.g., JobStatus, JobProfile, list of jobs in the queue, etc. Irrespective of this ACL configuration, (a) job-owner, (b) the user who started the cluster, (c) members of an admin configured supergroup configured via mapreduce.cluster.permissions.supergroup and (d) queue administrators of the queue to which this job was submitted to configured via acl-administer-jobs for the specific queue in mapred-queues.xml can do all the view operations on a job. By default, nobody else besides job-owner, the user who started the cluster, memebers of supergroup and queue administrators can perform view operations on a job.","Specifies whether ACLs should be checked for authorization of users for doing various queue and job level operations. ACLs are disabled by default. If enabled, access control checks are made by MapReduce ApplicationMaster when requests are made by users for queue operations like submit job to a queue and kill a job in the queue and job operations like viewing the job-details (See mapreduce.job.acl-view-job) or for modifying the job (See mapreduce.job.acl-modify-job) using Map/Reduce APIs, RPCs or via the console and web user interfaces. For enabling this flag, set to true in mapred-site.xml file of all MapReduce clients (MR job submitting nodes)."
36,mapreduce.job.token.tracking.ids.enabled,mapreduce.job.token.tracking.ids,"Whether to write tracking ids of tokens to job-conf. When true, the configuration property ""mapreduce.job.token.tracking.ids"" is set to the token-tracking-ids of the job","When mapreduce.job.token.tracking.ids.enabled is set to true, this is set by the framework to the token-tracking-ids used by the job."
37,mapreduce.job.token.tracking.ids,mapreduce.job.token.tracking.ids.enabled,"When mapreduce.job.token.tracking.ids.enabled is set to true, this is set by the framework to the token-tracking-ids used by the job.","Whether to write tracking ids of tokens to job-conf. When true, the configuration property ""mapreduce.job.token.tracking.ids"" is set to the token-tracking-ids of the job"
38,mapreduce.job.am.node-label-expression,mapreduce.job.node-label-expression,This is node-label configuration for Map Reduce Application Master container. If not configured it will make use of mapreduce.job.node-label-expression and if job's node-label expression is not configured then it will use queue's default-node-label-expression.,"All the containers of the Map Reduce job will be run with this node label expression. If the node-label-expression for job is not set, then it will use queue's default-node-label-expression for all job's containers."
39,mapreduce.map.node-label-expression,mapreduce.job.node-label-expression,This is node-label configuration for Map task containers. If not configured it will use mapreduce.job.node-label-expression and if job's node-label expression is not configured then it will use queue's default-node-label-expression.,"All the containers of the Map Reduce job will be run with this node label expression. If the node-label-expression for job is not set, then it will use queue's default-node-label-expression for all job's containers."
40,mapreduce.reduce.node-label-expression,mapreduce.job.node-label-expression,This is node-label configuration for Reduce task containers. If not configured it will use mapreduce.job.node-label-expression and if job's node-label expression is not configured then it will use queue's default-node-label-expression.,"All the containers of the Map Reduce job will be run with this node label expression. If the node-label-expression for job is not set, then it will use queue's default-node-label-expression for all job's containers."
41,mapreduce.job.end-notification.retry.attempts,mapreduce.job.end-notification.max.attempts,The number of times the submitter of the job wants to retry job end notification if it fails. This is capped by mapreduce.job.end-notification.max.attempts,"The maximum number of times a URL will be read for providing job end notification. Cluster administrators can set this to limit how long after end of a job, the Application Master waits before exiting. Must be marked as final to prevent users from overriding this."
42,mapreduce.job.end-notification.retry.interval,mapreduce.job.end-notification.max.retry.interval,The number of milliseconds the submitter of the job wants to wait before job end notification is retried if it fails. This is capped by mapreduce.job.end-notification.max.retry.interval,The maximum amount of time (in milliseconds) to wait before retrying job end notification. Cluster administrators can set this to limit how long the Application Master waits before exiting. Must be marked as final to prevent users from overriding this.
43,yarn.app.mapreduce.am.admin.user.env,yarn.app.mapreduce.am.env,"Environment variables for the MR App Master processes for admin purposes, specified as a comma separated list These values are set first and can be overridden by the user env (yarn.app.mapreduce.am.env). Example : 1) A=foo This will set the env variable A to foo 2) B=$B:c This is inherit app master's B env variable. To define environment variables individually, you can specify multiple properties of the form yarn.app.mapreduce.am.admin.user.env.VARNAME, where VARNAME is the name of the environment variable. This is the only way to add a variable when its value contains commas.","User added environment variables for the MR App Master processes, specified as a comma separated list. Example : 1) A=foo This will set the env variable A to foo 2) B=$B:c This is inherit tasktracker's B env variable. To define environment variables individually, you can specify multiple properties of the form yarn.app.mapreduce.am.env.VARNAME, where VARNAME is the name of the environment variable. This is the only way to add a variable when its value contains commas."
44,yarn.app.mapreduce.am.admin-command-opts,yarn.app.mapreduce.am.command-opts,Java opts for the MR App Master processes for admin purposes. It will appears before the opts set by yarn.app.mapreduce.am.command-opts and thus its options can be overridden user. Usage of -Djava.library.path can cause programs to no longer function if hadoop native libraries are used. These values should instead be set as part of LD_LIBRARY_PATH in the map / reduce JVM env using the mapreduce.map.env and mapreduce.reduce.env config settings.,"Java opts for the MR App Master processes. The following symbol, if present, will be interpolated: @taskid@ is replaced by current TaskID. Any other occurrences of '@' will go unchanged. For example, to enable verbose gc logging to a file named for the taskid in /tmp and to set the heap maximum to be a gigabyte, pass a 'value' of: -Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc Usage of -Djava.library.path can cause programs to no longer function if hadoop native libraries are used. These values should instead be set as part of LD_LIBRARY_PATH in the map / reduce JVM env using the mapreduce.map.env and mapreduce.reduce.env config settings."
45,mapreduce.fileoutputcommitter.task.cleanup.enabled,mapreduce.fileoutputcommitter.algorithm.version,"Whether tasks should delete their task temporary directories. This is purely an optimization for filesystems without O(1) recursive delete, as commitJob will recursively delete the entire job temporary directory. HDFS has O(1) recursive delete, so this parameter is left false by default. Users of object stores, for example, may want to set this to true. Note: this is only used if mapreduce.fileoutputcommitter.algorithm.version=2","The file output committer algorithm version valid algorithm version number: 1 or 2 default to 2, which is the original algorithm In algorithm version 1, 1. commitTask will rename directory $joboutput/_temporary/$appAttemptID/_temporary/$taskAttemptID/ to $joboutput/_temporary/$appAttemptID/$taskID/ 2. recoverTask will also do a rename $joboutput/_temporary/$appAttemptID/$taskID/ to $joboutput/_temporary/($appAttemptID + 1)/$taskID/ 3. commitJob will merge every task output file in $joboutput/_temporary/$appAttemptID/$taskID/ to $joboutput/, then it will delete $joboutput/_temporary/ and write $joboutput/_SUCCESS It has a performance regression, which is discussed in MAPREDUCE-4815. If a job generates many files to commit then the commitJob method call at the end of the job can take minutes. the commit is single-threaded and waits until all tasks have completed before commencing. algorithm version 2 will change the behavior of commitTask, recoverTask, and commitJob. 1. commitTask will rename all files in $joboutput/_temporary/$appAttemptID/_temporary/$taskAttemptID/ to $joboutput/ 2. recoverTask actually doesn't require to do anything, but for upgrade from version 1 to version 2 case, it will check if there are any files in $joboutput/_temporary/($appAttemptID - 1)/$taskID/ and rename them to $joboutput/ 3. commitJob can simply delete $joboutput/_temporary and write $joboutput/_SUCCESS This algorithm will reduce the output commit time for large jobs by having the tasks commit directly to the final output directory as they were completing and commitJob had very little to do."
46,yarn.app.mapreduce.client.job.retry-interval,yarn.app.mapreduce.client.job.max-retries,The delay between getJob retries in ms for retries configured with yarn.app.mapreduce.client.job.max-retries.,"The number of retries the client will make for getJob and dependent calls. This is needed for non-HDFS DFS where additional, high level retries are required to avoid spurious failures during the getJob call. 30 is a good value for WASB"
47,mapreduce.application.classpath,mapreduce.app-submission.cross-platform,"CLASSPATH for MR applications. A comma-separated list of CLASSPATH entries. If mapreduce.application.framework is set then this must specify the appropriate classpath for that archive, and the name of the archive must be present in the classpath. If mapreduce.app-submission.cross-platform is false, platform-specific environment vairable expansion syntax would be used to construct the default CLASSPATH entries. For Linux: $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*, $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*. For Windows: %HADOOP_MAPRED_HOME%/share/hadoop/mapreduce/*, %HADOOP_MAPRED_HOME%/share/hadoop/mapreduce/lib/*. If mapreduce.app-submission.cross-platform is true, platform-agnostic default CLASSPATH for MR applications would be used: {{HADOOP_MAPRED_HOME}}/share/hadoop/mapreduce/*, {{HADOOP_MAPRED_HOME}}/share/hadoop/mapreduce/lib/* Parameter expansion marker will be replaced by NodeManager on container launch based on the underlying OS accordingly.","If enabled, user can submit an application cross-platform i.e. submit an application from a Windows client to a Linux/Unix server or vice versa."
48,mapreduce.application.framework.path,mapreduce.application.classpath,"Path to the MapReduce framework archive. If set, the framework archive will automatically be distributed along with the job, and this path would normally reside in a public location in an HDFS filesystem. As with distributed cache files, this can be a URL with a fragment specifying the alias to use for the archive name. For example, hdfs:/mapred/framework/hadoop-mapreduce-2.1.1.tar.gz#mrframework would alias the localized archive as ""mrframework"". Note that mapreduce.application.classpath must include the appropriate classpath for the specified framework. The base name of the archive, or alias of the archive if an alias is used, must appear in the specified classpath.","CLASSPATH for MR applications. A comma-separated list of CLASSPATH entries. If mapreduce.application.framework is set then this must specify the appropriate classpath for that archive, and the name of the archive must be present in the classpath. If mapreduce.app-submission.cross-platform is false, platform-specific environment vairable expansion syntax would be used to construct the default CLASSPATH entries. For Linux: $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*, $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*. For Windows: %HADOOP_MAPRED_HOME%/share/hadoop/mapreduce/*, %HADOOP_MAPRED_HOME%/share/hadoop/mapreduce/lib/*. If mapreduce.app-submission.cross-platform is true, platform-agnostic default CLASSPATH for MR applications would be used: {{HADOOP_MAPRED_HOME}}/share/hadoop/mapreduce/*, {{HADOOP_MAPRED_HOME}}/share/hadoop/mapreduce/lib/* Parameter expansion marker will be replaced by NodeManager on container launch based on the underlying OS accordingly."
49,mapreduce.job.classloader.system.classes,mapreduce.job.classloader,"Used to override the default definition of the system classes for the job classloader. The system classes are a comma-separated list of patterns that indicate whether to load a class from the system classpath, instead from the user-supplied JARs, when mapreduce.job.classloader is enabled. A positive pattern is defined as: 1. A single class name 'C' that matches 'C' and transitively all nested classes 'C$*' defined in C; 2. A package name ending with a '.' (e.g., ""com.example."") that matches all classes from that package. A negative pattern is defined by a '-' in front of a positive pattern (e.g., ""-com.example.""). A class is considered a system class if and only if it matches one of the positive patterns and none of the negative ones. More formally: A class is a member of the inclusion set I if it matches one of the positive patterns. A class is a member of the exclusion set E if it matches one of the negative patterns. The set of system classes S = I \ E.",Whether to use a separate (isolated) classloader for user classes in the task JVM.
50,mapreduce.jobhistory.intermediate-user-done-dir.permissions,mapreduce.jobhistory.intermediate-done-dir,"The permissions of the user directories in ${mapreduce.jobhistory.intermediate-done-dir}. The user and the group permission must be 7, this is enforced.",
51,mapreduce.jobhistory.cleaner.interval-ms,mapreduce.jobhistory.max-age-ms,"How often the job history cleaner checks for files to delete, in milliseconds. Defaults to 86400000 (one day). Files are only deleted if they are older than mapreduce.jobhistory.max-age-ms.",Job history files older than this many milliseconds will be deleted when the history cleaner runs. Defaults to 604800000 (1 week).
52,mapreduce.jobhistory.loadedjobs.cache.size,mapreduce.jobhistory.loadedtasks.cache.size,Size of the loaded job cache. This property is ignored if the property mapreduce.jobhistory.loadedtasks.cache.size is set to a positive value.,"Change the job history cache limit to be set in terms of total task count. If the total number of tasks loaded exceeds this value, then the job cache will be shrunk down until it is under this limit (minimum 1 job in cache). If this value is empty or nonpositive then the cache reverts to using the property mapreduce.jobhistory.loadedjobs.cache.size as a job cache size. Two recommendations for the mapreduce.jobhistory.loadedtasks.cache.size property: 1) For every 100k of cache size, set the heap size of the Job History Server to 1.2GB. For example, mapreduce.jobhistory.loadedtasks.cache.size=500000, heap size=6GB. 2) Make sure that the cache size is larger than the number of tasks required for the largest job run on the cluster. It might be a good idea to set the value slightly higher (say, 20%) in order to allow for job size growth."
53,mapreduce.jobhistory.loadedtasks.cache.size,mapreduce.jobhistory.loadedjobs.cache.size,"Change the job history cache limit to be set in terms of total task count. If the total number of tasks loaded exceeds this value, then the job cache will be shrunk down until it is under this limit (minimum 1 job in cache). If this value is empty or nonpositive then the cache reverts to using the property mapreduce.jobhistory.loadedjobs.cache.size as a job cache size. Two recommendations for the mapreduce.jobhistory.loadedtasks.cache.size property: 1) For every 100k of cache size, set the heap size of the Job History Server to 1.2GB. For example, mapreduce.jobhistory.loadedtasks.cache.size=500000, heap size=6GB. 2) Make sure that the cache size is larger than the number of tasks required for the largest job run on the cluster. It might be a good idea to set the value slightly higher (say, 20%) in order to allow for job size growth.",Size of the loaded job cache. This property is ignored if the property mapreduce.jobhistory.loadedtasks.cache.size is set to a positive value.
54,mapreduce.jobhistory.recovery.enable,mapreduce.jobhistory.recovery.store.class,Enable the history server to store server state and recover server state upon startup. If enabled then mapreduce.jobhistory.recovery.store.class must be specified.,The HistoryServerStateStoreService class to store history server state for recovery.
55,mapreduce.outputcommitter.factory.scheme.s3a,mapreduce.outputcommitter.factory.class,"The committer factory to use when writing data to S3A filesystems. If mapreduce.outputcommitter.factory.class is set, it will override this property.","The name of an output committer factory for MRv2 FileOutputFormat to use for committing work. If set, overrides any per-filesystem committer defined for the destination filesystem."
